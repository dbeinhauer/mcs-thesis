\chapwithtoc{Introduction}

Significant advances in neurobiology have been made in recent decades. 
The development of new technologies and
methods has provided researchers with a diverse set of tools to
study the brain. With the rise of highly parallelized computing, 
computational neuroscience has become one of the most important
approaches to studying neuronal systems (\citet{trappenberg2009fundamentals}), 
offering a new perspective on
brain function. It has enabled the simulation of large-scale neuronal
networks, allowing us to analyze their behavior without relying solely
on real-world experimental data subject to various limitations. As a result, we can
now investigate brain systems in greater detail and gain a more precise
understanding of their underlying principles.

With the rapid expansion of machine learning, particularly deep neural
network (DNN) models, neuroscientists have also sought to apply these techniques.
State-of-the-art convolutional DNN models have demonstrated outstanding
performance in tasks such as image classification and object detection
(\citet{krizhevsky2012imagenet}, \citet{li2014medical}). 
These methods have also been used to model certain regions of the brain, often
yielding promising results. However, they come
with significant limitations (\citet{celeghin2023convolutional}).
Traditionally, researchers using DNNs typically disregard the anatomical structure
and constraints of real neuronal networks. For instance, they usually rely exclusively on feed-forward architecture, whereas real brain networks are highly recurrent. As a consequence these DNN models typically only predict the average response of neurons over a period of time and hence do not fully capture the complex non-linear dynamics of the biological neural networks. Ultimately, currently, ML approaches in neuroscience prioritize task performance over biological plausibility and, as a consequence, interpretability.

On the other hand, the usage of biologically plausible models such as
spiking neural network (SNN) models has been a fundamental approach in computational
neuroscience (\citet{ghosh2009spiking}, \citet{yamazaki2022spiking}). 
These models attempt to bridge biological knowledge with computational
methods by incorporating biologically relevant constraints, thus mechanistically explaining how the computations performed by the brain are implemented in the biological neural substrate. However, such SNN approaches face their own challenges, particularly limited ability to fit such models to data directly, therefore requiring a prior understanding of the system under study to generate precise mathematical formulations that define the behavior of the spiking network, limiting their flexibility and scalability (\citet{izhikevich2004model}).

One of the most studied, yet complex brain regions is the visual cortex.
Thanks to extensive experimental research, the cortical subregion, the first to process visual information that arrives from the retina, called the primary visual
cortex (V1), is the best understood (\citet{miikkulainen2006computational}). 
Extensive work on V1 involving both CNNs or SNNs have been undertaken, but both approaches have notable drawbacks (\citet{niell2021cortical}).

In this work, our objective is to overcome the limitations of both the CNN and SNN
approaches by integrating biological constraints into the design of recurrent neural network (RNN) models to better understand the primary visual cortex (V1). In this proof-of-concept thesis, we will rely on synthetic data generated by a SNN model of cat V1 developed by \citet{antolik2024comprehensive}, which we will use to train our novel RNN architectures. Specifically, we focus on predicting synthetic neuronal responses in layer IV and layers II and III of V1, the first two visual cortical processing stages, using input from lateral geniculate nucleus (LGN) neurons. 

While in this proof-of-concept initial study we train our model on synthetic SNN data, our long-term goal is to achieve strong predictive performance on real V1
neuronal recordings as well. Our novel V1 modeling RNN architecture incorporates the following biological constraints overlooked in previous ML models of visual system:

\begin{description}
\item[Anatomical structure alignment:] The architecture of our
model is layered according to known anatomical constraints. Each
neuron in the network corresponds to a specific neuron in the reference SNN system. 
Such one-to-one correspondence between the reference SNN and RNN system facilitates
straightforward principled validation of the ability of the novel RNN architectures to approximate the full dynamical complexity of the SNN systems. In future application to real data, such one-to-one correspondence would be dropped with the 
observable biological neurons linked to only small subset of the full RNN network.

\item[Excitatory and inhibitory neuron differentiation:] We explicitly
distinguish between excitatory and inhibitory neurons, enforcing
biologically plausible behavior within the architecture and ensuring
that specific neuronal types function as expected.

\item[Biologically inspired activation functions:] Instead of standard
activation functions such as ReLU or tanh, we introduce small 
DNN or RNN modules in place of each reference SNN neurons, that approximate the biological transfer function of the given neuron. The parameters of these transfer modules are shared across each given anatomically defined layer. These modules aim to approximate the complexity of biological neuronal transfer function that 
involves number of processes that cannot be accounted for by simple monotonic point non-linearities, such as adaptation. They allow us to capture these non-linearities and allow neurons to retain a form of memory that adapts to previous stimuli.

\item[Synaptic adaptation modules:] To model the plasticity of neuronal
synapses, we incorporate modules that adjust synaptic responses based on
increased stimuli rates from specific neuronal layers. This mechanism aims
to reflect the synaptic adaptation processes observed in biological
synapses.
\end{description}

By embedding these anatomical constraints into our model, we aim to
enhance both the predictive power and the interpretability of neural simulations,
ultimately contributing to a better understanding of the selected brain region.

In this thesis, we have obtained the following results. 
\begin{enumerate}
    \item Novel biologically constrained RNN architectures without DNN neuron modules can be trained to accurately capture the mean neuronal responses; however, their ability to reconstruct the full SNN dynamics remains limited. 
    \item Incorporating shared DNN modules for neurons slightly improved the model's ability to capture dynamic behavior. Nevertheless, predictions largely remained dominated by the mean neuronal responses rather than the full system dynamics. 
    \item Differentiating inputs from excitatory and inhibitory neurons to the neuronal module does not appear to significantly influence model performance.
    \item The use of RNN-based neuron modules substantially improved model performance in terms of capturing the temporal dynamics of neuronal responses and consistently outperformed all other tested model variants.
    \item The inclusion of the synaptic depression module did not improve model performance and, surprisingly, appeared to worsen the model dynamics. We primarily attribute this outcome to limited computational resources and suboptimal model parameter settings, leading to underfitting and reduced overall performance compared to its potential capabilities. 
\end{enumerate}

The thesis is structured into several sections. In the initial chapters, we introduce the theoretical background of neurobiology and computational neuroscience, with a focus on visual processing and, in particular, the primary visual cortex (V1). Alongside this, we review modeling approaches and machine learning methods relevant to our study. In the subsequent chapters, we describe in detail the architecture of our model, the dataset used, and the evaluation metrics employed. Finally, we present the experimental results of our study and provide an in-depth analysis of the findings.

\chapter{Methods}
\label{chap:methods}

In this chapter, we begin by describing the artificial stimulus dataset used to train our model, including a brief overview of the model from which the dataset originates. We then detail the preprocessing steps applied to the dataset to make it compatible with our model. The final and major section of this chapter focuses on our biologically plausible recurrent neural network (RNN) model of the primary visual cortex (V1).

\section{Spiking Model of Cat Primary Visual Cortex}
\label{sec:cats_model}
A major challenge in computational neuroscience, particularly in system identification tasks \ref{sec:system_identification}, is the limited availability and quality of biological data. This issue becomes even more pronounced when developing models using deep neural networks (DNNs), as discussed in Section \ref{subsec:deep_learning_approach}. The noise and expense of biological data acquisition motivate the use of artificial datasets. In our work, we utilize data generated by a state-of-the-art spiking model of the cat V1 developed by \citet{antolik2024comprehensive}.

Originally, we intended to fine-tune the model using real data from multielectrode array recordings in macaque V1, spanning tens of thousands of images. However, due to time constraints, this was not feasible within the scope of this thesis. Integration of real data remains a promising direction for future research.

\subsection{Spiking Model Description}
\label{subsec:spiking_model_description}

The artificial dataset is derived from a biologically detailed spiking model of cat V1 introduced by \citet{antolik2024comprehensive}. Designed to simulate visual experiments, this model aims to address the challenges inherent in biological data acquisition.

The model is implemented as a spiking neural network (SNN) using the exponential integrate-and-fire neuron model (\citet{FourcaudTrocm11628}), an extension of the leaky integrate-and-fire model (see Section \ref{subsec:spiking_neural_nets}). It captures the spatial layout of cortical layers IV and II/III in a 5.0 x 5.0 mm patch corresponding to the \emph{area centralis} of the retina, responsible for high-acuity vision (see Section \ref{subsec:retina}).

The model incorporates various biological properties, such as differentiation between excitatory and inhibitory neurons, synaptic depression, and local connectivity among neurons with similar orientation preferences (Section \ref{subsec:receptive_field}). It has been validated against a wide range of experimental data, including responses to white-noise and natural stimuli (Section \ref{subsec:stimulus}). Compared to other models, it is distinguished by its comprehensive inclusion of biological features (\citet{antolik2024comprehensive}).

A particularly notable strength of the model is its ability to reproduce spontaneous neuronal activity without artificial noise sources. This enables it to replicate trial-to-trial variability in stimulus responses (Section \ref{sec:evaluation_methods}), as demonstrated in \citet{baudot_animation_2013}. These capabilities were validated using blank gray stimuli (\citet{PAPAIOANNOU1972558}), making the model a valuable source of biologically plausible data.

Despite its strengths, the model remains a simplification of the biological system. For instance, feedback from cortical layers V and VI and the diversity of neuron types within layers are not explicitly modeled (Section \ref{sec:v1}). Furthermore, the recurrent pathway from layer II/III to IV serves as a proxy for these excluded modulatory circuits.

\subsubsection{Model Architecture}
\label{subsubsec:spiking_cat_architecture}
The model is substantially downsampled relative to the biological system. It comprises 108,150 neurons and approximately 155 million synapses, representing roughly 10\% of the neuronal density in cat V1 (\citet{beaulie1989number}). Neurons are distributed across layers according to biological data, with excitatory and inhibitory neurons present in a 4:1 ratio (\citet{bealuliee1992quantitative, markram_interneurons_2004}). The model includes both feedforward and recurrent connections within and between layers.

It also incorporates a simplified lateral geniculate nucleus (LGN) model (Section \ref{sec:lgn}), with distinct ON and OFF pathways (Figures \ref{fig:early_vis_processing} and \ref{fig:on_off_cells}). These 7,200 LGN neurons are spatially aligned with the receptive fields of their V1 counterparts. For further details, see \citet{antolik2024comprehensive}.

\section{Artificial Dataset}
\label{sec:artificial_dataset}
Our dataset consists of spiking activity generated by the model described above \ref{sec:cats_model}. It includes responses from 14,400 LGN neurons and 108,150 V1 neurons, recorded at 1 ms resolution. The data is organized into sequences corresponding to multiple experiments, each comprising alternating blank and natural image stimuli. Due to the model's comprehensiveness, unnecessary information was removed during preprocessing, as described below.

\subsection{Stimulus Sequence Structure}
\label{subsec:stimulus_sequence}
The dataset is divided into \emph{stimulus sequences}, each comprising alternating intervals of a blank gray screen and a natural stimulus image neuronal responses. Each pair forms a \emph{stimulus pair}, where the blank interval precedes the stimulus to ensure that only spontaneous activity is present before the image is shown.

Each stimulus sequence begins with a blank interval of duration $t_{\text{blank}}$, followed by a stimulus interval of duration $t_{\text{stim}}$. These pairs are repeated periodically. Blank intervals serve to reset neural activity, preventing carryover effects from previous stimuli. Notably, the final blank interval following the last stimulus is omitted, introducing a minor inconsistency.

\subsection{Experiment Definition}
\label{subsec:experiment}
To facilitate processing and reduce memory demands, stimulus sequences are divided into smaller units called \emph{experiments}. Each experiment consists of the second half of a blank interval, the corresponding stimulus interval, and the first half of the following blank interval. This ensures that the neural activity at the boundaries of the stimulus reflects spontaneous states.

\begin{defn}[Experiment]
    Let $S$ be a stimulus sequence consisting of $N \in \mathbb{N}$ pairs of blank and stimulus intervals $(b_i, s_i)$. Let $l_b$ and $l_s$ be the lengths of the blank and stimulus intervals, respectively. Then, for $j \in [1, 2, \dots, N-1]$, an experiment is defined as:

    $$
    \left(b_j\left[\frac{l_b}{2}:l_b\right]; s_j; b_{j+1}\left[0:\frac{l_b}{2}\right]\right)
    $$

    The final experiment omits the trailing blank interval:

    $$
    \left(b_N\left[\frac{l_b}{2}:l_b\right]; s_N\right)
    $$
\end{defn}
\label{def:experiment}

To maintain consistency, the first half of the initial blank interval is omitted. While this results in minimal data loss, it facilitates uniform formatting. Experiments lacking a post-stimulus blank interval are padded with zeros.

\subsection{Dataset Preprocessing}
\label{subsec:dataset_preprocess}
The original dataset includes detailed simulation data, much of which is irrelevant for training. We extract only the spike trains (Section \ref{subsec:spike_trains}) and segment them into experiments, reducing memory and computation demands significantly.

To streamline this process, we have developed a dedicated data processing pipeline that converts raw simulation output into the final experiment-based format used by our model. This pipeline is modular and reusable, allowing efficient processing of future datasets generated by the same spiking model. As such, it facilitates further experiments and model iterations without redundant implementation effort.

\subsection{Dataset Description}
\label{subsec:dataset_description}

After the raw dataset generated from the model by \citet{antolik2024comprehensive} undergoes the preprocessing steps described in Section \ref{subsec:dataset_preprocess}, we obtain the final dataset used in our model. The basic unit of this dataset is the experiment (Section \ref{subsec:experiment}). Each experiment is represented by six matrices corresponding to different neuronal populations. Two matrices represent the spike trains of ON and OFF LGN neurons (denoted as $s_{ON}$ and $s_{OFF}$), while the remaining four correspond to layer IV and layer II/III neurons, further divided into excitatory and inhibitory types: $r_{E_{4}}$, $r_{I_{4}}$, $r_{E_{2/3}}$, and $r_{I_{2/3}}$. The number of neurons in each population is defined in Section \ref{subsubsec:spiking_cat_architecture}, matching the configuration in \citet{antolik2024comprehensive}.

Each matrix has the format $\mathbb{R}^{N \times M \times T}$, where $N$ is the number of experiments, $M$ is the number of neurons in the given population, and $T$ is the number of time steps in each experiment (see Section \ref{subsec:experiment} for experiment definition).

In our artificial dataset, the blank stage lasts for 150 ms, and the natural stimulus stage lasts for 560 ms, both recorded in 1 ms bins. Thus, the spike train for a single neuron in an experiment spans 710 ms, composed of 75 ms of a blank interval before the stimulus, 560 ms of stimulus presentation, and 75 ms of the subsequent blank interval. The spike train is encoded in binary format: a 1 indicates a spike at a given time step, and a 0 indicates no spike. For some experiments at the end of a stimulus sequence, the final blank interval is missing. To ensure uniform experiment duration, we pad these with zeros (no spikes).

Although this padding does not accurately reflect biological spontaneous activity (which is non-zero, see Section \ref{subsec:spiking_model_description}), it affects only a minority of cases. We chose this pragmatic solution for its simplicity and minimal impact on model performance, avoiding additional complexity and computational overhead.

In classical machine learning, datasets are typically split into training, validation, and test subsets, often with randomized sampling. However, our use case is complicated by trial-to-trial variability in neuronal responses, necessitating multiple repeated trials in the validation and test datasets (see Section \ref{sec:evaluation_methods}). Because this variability arises mainly from spontaneous neural activity (\citet{antolik2024comprehensive}), we chose not to include multiple trials in the training set. Instead, we prioritized generating a wider variety of single-trial responses to diverse natural stimuli. This design choice balances several practical factors: dataset size, memory efficiency, training speed, and simulation time.

As a result, we do not apply randomized splits across the dataset but instead rely on a structured division between training and evaluation datasets.

\subsubsection{Train Dataset}
\label{subsubsec:train_dataset}

The training dataset consists of 500 stimulus sequences, each containing 100 pairs of blank and natural stimuli, all presented in a single trial. After preprocessing and experiment segmentation (Section \ref{subsec:experiment}), this yields a total of 50,000 unique experiments for training.

\subsubsection{Test and Validation Dataset}
\label{subsubsec:test_dataset}

The validation and test datasets comprise 18 stimulus sequences, each with 50 stimulus pairs repeated over 20 trials. After preprocessing, this results in 900 experiments, each with 20 trial repetitions. We randomly divide this set into validation and test subsets using a 1:9 ratio.

\subsubsection{Neuron Subset Selection}
\label{subsubsec:subset_selection}

To build a biologically plausible RNN model that mirrors the structure in \citet{antolik2024comprehensive}, one would ideally create a one-to-one mapping between model neurons and biological neurons. However, given the lack of structural constraints (e.g., known synaptic weights) and the impracticality of modeling over 100,000 neurons, we adopt an all-to-all connectivity scheme with neuron subsampling.

We randomly select 10\% of neurons from each population to reduce model complexity and memory consumption, making the model more tractable during training and fine-tuning. To mitigate sampling bias, we run experiments using 20 different random neuron subsets. Empirical testing has shown that using this 10\% subset produces performance metrics similar to those obtained with larger neuron subsets. Further evaluation of this design choice is presented in the next chapter.

\section{Model Description}
\label{sec:model_description}

This section describes the core model used in our study, followed by the biologically inspired enhancements we applied to improve its performance.

Our goal is to develop a model for visual system identification task, as introduced in Section~\ref{sec:system_identification}. Specifically, we develop a recurrent neural network (RNN) that takes in time-series data from LGN neurons (detailed in Section~\ref{sec:lgn}) and predicts the activity of selected V1 neurons. Both input and target output data come from the artificial dataset described in Section~\ref{sec:artificial_dataset}.

At every time step $t$ in the experimental sequence (Section~\ref{subsec:experiment}), the model receives a vector of inputs representing responses from LGN ON and OFF cells. We denote this vector as $s(t)$:

\begin{equation*}
    s(t) = \left(s_{ON}(t); s_{OFF}(t)\right)
\end{equation*}

Our objective is to learn a biologically plausible function $f$ implemented via an RNN, which maps the LGN input to predicted V1 responses:

\begin{equation*}
    \hat{r}(t) = f(s(t))
\end{equation*}

The output vector $\hat{r}(t)$ includes predictions for four types of V1 neuron populations:

\begin{equation*}
    \hat{r}(t) = \left(\hat{r}_{E_4}(t); \hat{r}_{I_4}(t); \hat{r}_{E_{2/3}}(t); \hat{r}_{I_{2/3}}(t)\right)
\end{equation*}

In this text, we use a hat symbol $(\hat{.})$ to indicate predicted values, while unmarked variables refer to the ground truth values from our dataset.

The main learning goal is to minimize the difference between predicted responses $\hat{r}(t)$ and actual responses $r(t)$, while accurately modeling the temporal and spatial dynamics of each neuron. As noted in Section~\ref{sec:evaluation_methods}, there is no universally accepted evaluation metric for this problem. For our analysis on the test dataset (Section~\ref{subsubsec:test_dataset}), we use Pearson's correlation coefficient (Section~\ref{subsec:pearson_cc}) and normalized cross-correlation (Section~\ref{subsec:normalized_cross_correlation}) to evaluate model performance.

For training, we use the mean squared error (MSE) loss function (\citet{alpaydin2020introduction}). The model is trained by minimizing the following objective function during backpropagation:

\begin{equation*}
    L(\hat{R}, R) = \sum_{i=1}^{N}\sum_{t=1}^{T}\left(\hat{R}_i(t) - R_i(t)\right)^2
\end{equation*}

Here, $\hat{R}, R \in \mathbb{R}^{N \times M_{out} \times T}$ represent the predicted and actual response matrices, where $N$ is the number of training experiments, $M_{out}$ is the number of output neurons, and $T$ is the total number of time steps.

Choosing an appropriate loss function for visual system identification is still an open question in the field. We chose MSE because it is widely used, easy to interpret, and simple to implement. It has also been effective in many machine learning and signal processing applications (\citet{wang2009mse, soderstrom2018errors}), including prior neuroscience modeling studies (\citet{antolik2016local}).

That said, other loss functions such as the Poisson loss are often more suitable for modeling neural spike data, since these outputs represent discrete counts. Several recent studies (\citet{terven2024lossfunctionsmetricsdeep, Wang2023towards, sinz2018stimulus}) have used Poisson loss for this reason. Although Poisson loss may provide more biologically accurate modeling, we chose MSE due to its strong performance in our early trials and the practical advantages it offers. We recognize that this choice may have some limitations and suggest testing alternative loss functions in future research.

To optimize the model, we use the Adam optimizer (\citet{kingma2017adammethodstochasticoptimization}), a widely used method in deep learning known for its fast convergence and reliability.

\subsection{Base Model Architecture}
\label{subsec:base_model_architecture}

This section introduces the base model architecture, which serves as the foundation for later enhancements aimed at improving both model performance and biological realism.

We refer to this initial version as the \emph{simple model}, and it acts as the starting point for further development throughout this study.

The simple model is a recurrent neural network (RNN) designed to approximate the structure and function of layers IV and II/III of the primary visual cortex (V1). Its design is inspired by the spiking neural network (SNN) described in~\citet{antolik2024comprehensive}, which also provides the artificial dataset used for training and evaluation. Because fully replicating all biological details is computationally intensive and beyond the scope of this thesis, we introduce several simplifications that maintain key structural aspects while keeping the model tractable. Our long-term objective is to integrate this RNN with the original SNN to build a more comprehensive simulation framework.

The model takes as input a sequence of spiking responses from LGN neurons, represented by vectors $s_{ON}$ and $s_{OFF}$, corresponding to ON and OFF LGN cell populations (see Figure~\ref{fig:on_off_cells} and Section~\ref{sec:lgn}). These inputs are fed into two RNN layers that model the excitatory ($E_4$) and inhibitory ($I_4$) subpopulations of layer IV in V1. Each artificial neuron is designed to correspond to a neuron in the original SNN, though we use only a subset of these due to memory constraints (see Section~\ref{subsubsec:subset_selection}).

Within layer IV, $E_4$ and $I_4$ are fully interconnected and also include full self-connections. The output from $E_4$ is passed to two additional RNN layers that represent the excitatory ($E_{2/3}$) and inhibitory ($I_{2/3}$) subpopulations of layer II/III. These layers follow the same full connectivity and self-connectivity pattern as layer IV. Additionally, we include a recurrent feedback connection from $E_{2/3}$ to both $E_4$ and $I_4$. This simplifies the feedback typically mediated by deeper cortical layers (V and VI), as described in~\citet{antolik2024comprehensive}.

We define the operation of each RNN layer as follows:

\begin{defn}[Base Neuronal Layer]
    Let $L$ be a layer with $m_h \in \mathbb{N}$ neurons. Let $x \in \mathbb{R}^{m_{in}}$ be the input vector, where $m_{in} \in \mathbb{N}$ is the number of input neurons. Let $W_{ih} \in \mathbb{R}^{m_h \times m_{in}}$ and $b_{ih} \in \mathbb{R}^{m_h}$ be the input weight matrix and bias vector. Let $h \in \mathbb{R}^{m_h}$ be the previous hidden state of the layer, with recurrent weights $W_{hh} \in \mathbb{R}^{m_h \times m_h}$ and bias $b_{hh} \in \mathbb{R}^{m_h}$. Let $f: \mathbb{R}^{m_h} \to \mathbb{R}^{m_h}$ be an activation function. Then the updated hidden state $h'$ is calculated as:
    
    $$h' = f\left(W_{ih}x + b_{ih} + W_{hh}h + b_{hh}\right)$$
\end{defn}
\label{def:base_neuron}

The activation function $f$ can be any non-linear function that operates element-wise. In this thesis, we often use vector notation for clarity. Unless otherwise stated, $f$ is applied independently to each element of the vector.

This model architecture is loosely based on the biological structure of V1 (see Section~\ref{sec:v1}) and incorporates similar simplifications found in the original SNN~\citep{antolik2024comprehensive}. Our main simplifications include:
\begin{itemize}
    \item Modeling only a randomly chosen subset of neurons, not the full population.
    \item Using all-to-all connectivity rather than spatially constrained, probabilistic connections.
\end{itemize}

In contrast, the original SNN uses a connection probability that decreases with spatial distance and considers features like orientation selectivity. While this simplification speeds up computation and reduces model complexity, it reduces biological realism. Future work could reintroduce spatial constraints to make the model more scalable and biologically accurate.

At each time step $t$, the model's output consists of the concatenated outputs from all four RNN layers:

$$\hat{R}(t) = (E_4(t); I_4(t); E_{2/3}(t); I_{2/3}(t))$$

While the output could potentially be interpreted as categorical, we frame our task as a regression problem. This choice reflects current practice in the field and acknowledges the lack of standardized loss and evaluation metrics for neural response prediction tasks.

\subsubsection{Time Bins Merging}
\label{subsubsec:time_bins_merging}

Training neural networks with millisecond-resolution data is both time- and memory-intensive, especially when using single-trial recordings, which are inherently noisy. After inspecting the dataset and considering the trade-off between temporal detail and computational efficiency, we decided to aggregate the data into 20~ms bins instead of 1~ms. This change preserves meaningful temporal patterns in neural activity while significantly reducing data volume and smoothing out noise. While this binning reduces high-frequency detail, our data showed low spike rates overall, making this trade-off acceptable. The impact of this decision is further discussed in the results section.

\subsubsection{LeakyTanh Activation Function}
\label{subsubsec:leakytanh}

As mentioned in Definition~\ref{def:base_neuron}, the model architecture allows flexibility in choosing the activation function. For our baseline RNN, we use a custom activation function called \emph{LeakyTanh}, which was developed by Richard Kraus as part of his bachelor's thesis, which extends this work.

LeakyTanh was designed to produce biologically plausible outputs while preserving stable gradients during training. The goal was to create an activation function suitable for predicting firing rates, which should be non-negative. Given our 20~ms binning and the theoretical upper bound of 20 spikes per bin (based on a maximum rate of one spike per millisecond~\citep{dayan2005theoretical}), it makes sense for the activation function to output values in a narrow, biologically meaningful range. However, our dataset rarely shows more than four spikes per bin, and most responses are binary, so even lower outputs are typically sufficient.

We tested several activation functions, including ReLU, tanh, and custom variants. LeakyTanh provided the most stable training behavior. We believe this is because it is fully differentiable across $\R$ and unbounded on the upper end, while still being partially constrained from below. Functions that are bounded on both ends performed poorly~\citep{shiv2022activation, nwankpa2018activationfunctionscomparisontrends}, and fully unbounded functions caused issues with exploding gradients, even when gradient clipping was applied. Although we still use gradient clipping with LeakyTanh, its design generally avoids the need for it.

The LeakyTanh function is defined as follows:

\begin{defn}[LeakyTanh]
    Let $[0, m]$ be the target output range. Define $c = \frac{m}{2}$ as the central offset, $s \in \mathbb{R}$ as the steepness parameter for the $\tanh$ core, and $l \in \mathbb{R}$ as the leakage factor. We also use the softplus function $\text{softplus}(x, \beta)$ with steepness parameter $\beta \in \mathbb{R}$ as a leakage factor. The LeakyTanh function is defined as:
    
    $$\text{LeakyTanh}(x; c, s, l, \beta) = c + c \cdot \tanh(s \cdot x) + l \cdot \text{softplus}(x, \beta)$$
\end{defn}
\label{def:leakytanh}

In our experiments, we used $c = 2.5$ (centering outputs around the 0--5 range), $l = 0.001$, $s = 0.2$, and $\beta = 20$. These parameters were fine-tuned by Richard Kraus. Our contribution focused on analyzing the dataset to define the desired output characteristics and comparing the performance of various activation functions in our model.

\subsubsection{Excitatory/Inhibitory Neurons Differentiation}
\label{subsubsec:exc_inh_differentiation}

To increase biological realism, we differentiate excitatory and inhibitory neurons by applying sign constraints to their weights (see Section~\ref{subsec:synaptic_transmission}). Neurons in excitatory layers ($E_4$ and $E_{2/3}$) are constrained to have non-negative weights, promoting excitatory (amplifying) effects. Inhibitory layers ($I_4$ and $I_{2/3}$) are constrained to have non-positive weights, modeling their suppressive role in the network.

These constraints are enforced after every optimization step. If any updated weight violates the constraint, it is clipped to the appropriate boundary.

Formally, the constraints are applied as follows:

\begin{defn}[Weight Constraints]
    Let $w_E, w_I \in \mathbb{R}$ be weights from excitatory and inhibitory neurons, respectively. After each update step, we apply the following rules:
    
    \textbf{Excitatory neurons:}
    \begin{equation*}
        w_{E_{\text{new}}} =
        \begin{cases}
            w_E & \text{if } w_E \geq 0 \\
            0 & \text{otherwise}
        \end{cases}
    \end{equation*}
    
    \textbf{Inhibitory neurons:}
    \begin{equation*}
        w_{I_{\text{new}}} =
        \begin{cases}
            w_I & \text{if } w_I \leq 0 \\
            0 & \text{otherwise}
        \end{cases}
    \end{equation*}
\end{defn}
\label{def:weight_constraints}


\subsubsection{Training of the Model}
\label{subsubsec:training_model}

The training of the model was done the following way. At the beginning we pass the appropriate input values to the network starting with the processing in layer IV. As this layer apart from the LGN neuronal responses works with the recurrent input from the other cortical layers we pass these values from our artificial dataset from the previous time steps. This we apply for all the recurrent connections in the model. We perform one forward step, compute the loss between the neuronal predictions in each layer, perform backpropagation step and optimizer step and at the end we apply the weights constraints to fulfill the excitatory/inhibitory differentiation \ref{subsubsec:exc_inh_differentiation}. After the model weight update we do not pass the model outputs as the recurrent inputs for the next time step, but we reset these values with the corresponding target value from the artificial dataset. This ensures the model gets the correct input values in each time step and also speeds up the training. As we do not really have any hidden parameters in the simple model and we know exactly how should each neuron in our model respond on all the stimuli, we can exploit this non-typical property of the RNN training for facilitation of the model training. Since we are working with the RNN model it also makes sense to try to apply the hidden time steps. On the other words, to perform multiple forward steps and take the last time step as the model predictions in the target time step. This variant makes it possible to use the backpropagation through time (\citet{webos1990btt}) and have some kind of memory encoded in the model. We have implemented this option, but while testing it did not show any promising results and we won't apply this option in the future study. It is worth noting that in the future this functionality might be useful and might improve the model performance.

\subsubsection{Evaluation of the Model}
\label{subsubsec:evaluation_model}

The evaluation of the model is simple though. We initiate the recurrent inputs with the first time step targets from our artificial dataset and then perform the forward steps for all the time steps when taking only the LGN inputs and not resetting the recurrent inputs anymore. This means that during the evaluation we can test whether our model is able to predict the whole series of the time steps in the experiment, eventhough, it was never really presented to the model during the training.

\subsection{Additional Modules}
\label{subsec:additional_modules}
Apart from the simple model described in the Section \ref{subsec:base_model_architecture} we added multiple additional modules that enhance the model performance and architecture using additional biological constraints. We will describe the modules in the order in which we was adding the modules to improve the model performance. So, the reader can see the process of the model development and the ideas behind each of the module addition. In theory it does not refrain the user from omitting the previously defined modules but it typically does not make sense as they were build to fine-tune the previous version of the model.

\subsubsection{Feed-forward Neuron Module}
\label{subsubsec:dnn_neuron}
The first and arguably also the most impactful complement is the replacement of LeakyTanh neuron activation functions of the simple model using the shared small trainable feed-forward DNN modules that should better reflect the complex behavior of the neuron. We use 4 separate DNN modules for each output neuronal population layer ($E_4$, $I_4$, $E_{2/3}$, $I_{2/3}$). This should also differentiate different neuronal types in the system. As in the source model of our artificial dataset the other types are omitted thus the model is simplified by this. At the end of each of these modules there is a final LeakyTanh activation function to maintain the properties of the predictions mentioned in the Section \ref{subsubsec:leakytanh}.

The feed-forward DNN neuron module is defined as:

\begin{defn}[DNN Joint Neuron Module]
    Let $n \in \N$ be a number of module layers and $s \in \N$ be a size of a single layer. Let there be a sequence of the following operations Layer Normalization (\citet{ba2016layernormalization}), fully-connected hidden NN layer and ReLU. This sequence represents one layer with exception to the first layer. The input size of the DNN joint neuron module is 1 as it is the output element of the vector of the base model defined in \ref{def:base_neuron}. Then the DNN neuron module $f: \R \to \R$ is defined as the sequence of operations:
    \begin{enumerate}
        \item Fully connected NN layer with input size $1$ and output size $s$.
        \item Sequence of $(n-1)$ repetitions of the layer block with operations:
            \begin{enumerate}
                \item Layer Normalization.
                \item Fully connected NN layer with input size $s$ and output size $s$.
                \item ReLU
            \end{enumerate}
        \item Fully connected NN layer with input size $s$ and output size $1$.
        \item LeakyTanh final activation function.
    \end{enumerate}

    Optionally, there might be a residual connection from that skips the whole module and is summed with the module result before the final activation function is applied.
\end{defn}
\label{def:dnn_joint}

Since now we will name model using this feed-forward neuronal module as \emph{DNN joint model}. The reason for the "joint" adverb is that this type of module does not differentiate excitatory and inhibitory outputs of the base RNN defined in the Section \ref{subsec:base_model_architecture} marked as simple model.

The idea behind this module is to capture the different behavior of different neuronal populations. Alongside with this there is also a motivation to learn more complex activation function than the simple ones that are typically used in the DNN model.

\subsubsection{Splitting Excitatory/Inhibitory Output in Base RNN}
\label{subsubsec:dnn_separate}
We will call another extension of our model as \emph{DNN Separate model}. This model is basically the same as the one as the DNN joint \ref{subsubsec:dnn_neuron} with one modification that is separation of the excitatory and inhibitory outputs during the base model RNN computation defined in \ref{def:base_neuron}. In the separate models we will separate the linear RNN cell computation into 2 part one part that performs the linear step for all excitatory input neurons and the other part that does the same for the inhibitory neurons. The result of the linear base RNN neuron is the vector with 2 elements (inhibitory and excitatory). These 2 elements are then passed together to the DNN neuron module basically the same as used in the DNN joint model defined in \ref{def:dnn_joint}. The only difference is that the module takes the input of size 2 (excitatory and inhibitory part) and in case of residual connection it first sums both input elements and then it sums this sum before the final activation function application. The difference between the definition \ref{def:dnn_joint} is then that the DNN separate module is a function $f: \R^2 \to \R$.

The idea behind this extension is to facilitate the model to distinguish between the excitatory and inhibitory neurons and separate their functionality from each other. The formal definition of the base model using "separate" variant is the following:

\begin{defn}[Separate Base RNN]
    Let's $L$ be an arbitrary neuronal layer containing $m_h \in \N$ neurons. Let $x_E \in R^{m_{in_E}}$ and $x_I \in R^{m_{in_I}}$ be input neuronal responses from the excitatory respectively inhibitory neurons of the layer $L$ and $m_{in_E}, m_{in_I} \in \N$ sizes of these populations. Also let $W_{ih_E} \in \R^{m_h \times m_{in_E}}$, $W_{ih_I} \in \R^{m_h \times m_{in_I}}$, $b_{ih_E} \in \R^{m_{in_E}}$ and $b_{ih_I} \in \R^{m_{in_I}}$ be input weights matrices and bias vectors of the excitatory respectively inhibitory appropriate input neurons. Let $h \in \R^{m_h}$ be the vector of neuronal responses for each neuron from layer $L$ from the previous time step and its corresponding weights $W_{hh} \in \R^{m_h \times m_h}$ and bias $b_{hh} \in \R^{m_h}$. Let's mark $f: \R^{2 \times m_h} \to \R^{m_h}$ to be an arbitrary activation function. Then vector of neuronal responses $h'$ of the layer $L$ on the input responses $x_E$, $x_I$ and $h$ is defined as:

    \begin{equation*}
        h' = 
        \begin{cases}
            f\left(W_{ih_E}x + b_{ih_E} + W_{hh}h + b_{hh}; W_{ih_I}x + b_{ih_I}\right) & \text{if $L$ is excitatory layer}\\
            f\left(W_{ih_E}x + b_{ih_E}; W_{ih_I}x + b_{ih_I} + W_{hh}h + b_{hh}\right) & \text{if $L$ is inhibitory layer}\\
        \end{cases}      
    \end{equation*}
\end{defn}
\label{def:separate_base_rnn}

As mentioned before as the activation function we use the same neuronal DNN module as in the DNN joint model with the slight modification that is that it takes input of size $2$ and sums the input excitatory and inhibitory values across in case of residual connection. We won't define this formally as the definition would be almost the same as in case of \ref{def:dnn_joint}.

\subsubsection{Recurrent Neuron Module}
\label{subsubsec:rnn_neuron_module}
Another extension we call \emph{RNN joint} or \emph{RNN separate} based on whether we use joint \ref{def:base_neuron} or separate base \ref{def:separate_base_rnn} model. This extension aims to bring some form of a "memory" of the neurons. The principle is the same as in the feed-forward DNN neuronal modules \ref{subsubsec:dnn_neuron} and \ref{subsubsec:dnn_separate} we just replace the feed-forward DNN with the RNN variant. As the small shared RNN module we use either LSTM (\citet{hochreiter1997lstm}) or GRU (\citet{cho2014gru}) RNN model variant. Formally, the RNN neurons are defined as:

\begin{defn}[RNN Neuron Module]
    Let $n \in \N$ be a number of layers of either LSTM or GRU units and $s \in \N$ be a size of a single layer. Let the input size of the module be $s_i \in \{1, 2\}$ to be $1$ in case of joint model variant and $2$ in case of the separate variant. Let $h \in \R^{m_h}$ be a vector of hidden states of the module where $m_h \in \N$ is given by the variant of the RNN section (either LSTM or GRU). The RNN neuron is then the given sequence of operations:
    \begin{enumerate}
        \item Apply fully connected NN with input size $s_i$ and output size $m_h$.
        \item Apply multi layer RNN variant based on our choice. Let the final hidden states mark as $h_{out} \in \R^{m_h}$.
        \item Apply final fully connected layer with input size $m_h$ and output size $1$.
        \item Apply final LeakyTanh activation function.
    \end{enumerate}

    Optionally, same as in the DNN neuron variant we might apply the residual connection that skips the module and adds the input before the final activation function call.
\end{defn}
\label{def:rnn_neuron_module}

The motivation behind this extension is the fact that the neurons adapt their behavior based on the short-term dynamics. Thus this might lead to better capturing temporal dynamics of the overall network spiking.

However, the introduction of the RNN neurons raises the concern about the training procedure and there needs to be some modifications done during the training as these models captures the temporal dynamics only when applying backpropagation through time (BPTT) (\citet{webos1990btt}). This problem has been already mentioned in the Section \ref{subsubsec:training_model} as our base model is also designed on the RNN architecture. In the mentioned case there, however, was not the necessity for BPTT as we in fact know all the hidden states in all time steps and we can reset the hidden states using the values from our dataset as we also have biological explanation of the base RNN hidden states. Although, this is not the case while using the neuronal modules. These are the new aspects in our system and we do not know what should be the ideal hidden values throughout the experiment sequence. We thus need to modify our training procedure to allow the model to perform the BPTT.

The modification to the training procedure described in \ref{subsubsec:training_model} is fairly simple. The only difference is that we will perform the backpropagation computation and the optimizer step only after the given number $T_{\text{back}} \in \N$ of forward steps representing the system neuronal responses in $T_{\text{back}}$ time steps and after this we perform the backpropagation computation, optimizer step and excitatory/inhibitory neurons constraint application. After this we do this repeatedly till the experiment sequence is not over. We perform the backpropagation computation only till the time step where the last optimizer call happened. We do not reset the hidden states of the RNN shared modules, however, we still reset the states of the base RNN model using the known neuronal responses as before. It is worth mentioning that in the original BPTT there should be all forward steps in the sequence performed and then there should be only one BPTT call as described before. However, as we want to capture mainly the short-term dynamics and as with the longering the time sequence for BPTT is significantly memory and time efficient that is our big concern as mentioned in the previous text. We cut the backpropagation step only for the given number of time steps. This variant is also called truncated backpropagation through time (TBPTT) (\citet{Williams1990tbptt}).

It is also worth noting that since we do not apply the optimizer time steps after each time step the training itself also takes more time and also as mentioned before especially memory consumption of this variant rises significantly with the number of time steps that we want to perform while using TBPTT. We thus need to find the balance between the efficiency and the temporal resolution. In this step there significantly rises the necessity to apply some kind of spatial constraint in neuronal connections to reduce number of model parameters that there currently is due to the all-to-all connections as mentioned in the Section \ref{subsec:base_model_architecture}. These constraints are also present in the original template SNN from \citet{antolik2024comprehensive} and are the case of the parallel research that should focus on the improvement of our model and is out of the scope of our thesis.

\subsubsection{Synaptic Depression}
\label{subsubsec:synaptic_depression}
The last extension of the model aims to include even more network short-term temporal adaptability to the model while using the principles of the synaptic depression (\citet{abbott1997syndepression}) that are also included in our template model by \citet{antolik2024comprehensive}. The idea of this principle is quite simple. The input neuronal connections adapt to the increased or decreased firing rate in the dynamical changes of the stimuli and thus modulate the postsynaptic signal that is transmitted from one neuron to other for example by amount of neurotransmitter efflux etc. (\ref{subsec:synaptic_transmission}).

In our model we apply this principle using the similar approach as in the shared modules of the neurons introduced in the previous extensions. We would have a small shared RNN module across the same ordered synaptic pairs. This will ensure that connection between each layers are independently modulated in time based on the stimuli. The highest influence of this phenomenon in biological network is in the synapses from LGN neurons to layer IV neurons. So, we first introduce the model that we call \emph{LGN only synaptic adaptation model} that applies synaptic depression only on the connections from LGN. The extension of this model is the one that we call \emph{synaptic adaptation model} that applies this principle to each synaptic pair population. In the model the synaptic depression module is applied as a preprocessing step of the input to the layer.

Formally the synaptic depression module is defined as:

\begin{defn}[Synaptic Depression Module]
    Let there be layer of neuronal population $L$ that takes input from neuronal populations from the set $\Lambda_{L}$. Let there be an arbitrary input layer $L_{in} \in \Lambda_{L}$ and let $x \in \R^{m_{in}}$ be a vector of neuronal responses of the layer $L_{in}$ in time $t$ where $m_{in} \in \N$ is a size of the layer. Let there $\sigma: \R^{m_{in}} \to \R^{m_{in}}$ be an synaptic depression module that fulfills the definition of an RNN joint module from the definition \ref{def:rnn_neuron_module}. Then input of the layer $L$ in time $t$ from the layer $L_{in}$ after application of the synaptic depression module is:
    $$x' = \sigma(x)$$
\end{defn}
\label{def:synaptic_depression}

As it was already mentioned in the Section concerning the RNN neuron modules \ref{def:rnn_neuron_module} there needs to be TBPTT applied as the synaptic depression modules are basically the same as the RNN neuron modules. Furthermore, the problem of the memory and computational cost is in case of this model even stronger since the number of parameters increased and thus with each additional time step in TBPTT the number memory usage increase rapidly. This is also reason why we use the smallest as possible GRU variants of the RNN modules as they are using less parameters and thus are less memory consuming. Furthermore, the LGN only synaptic depression module was developed due to this problems as it significantly reduces the number of synaptic depression modules and thus does not increase the memory usage that significantly. Overall this module seems to be problematic to use in a current state of our model as with the all-to-all connection it is not very feasible to train such a model in a reasonable time, scale and memory usage. However, we have high expectations that this module might have severe influence in the overall quality of the model temporal predictions and with the application of the spatial connectivity constraints this extension might significantly improve the model.
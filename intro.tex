
\chapwithtoc{Introduction}

Significant advancements in neurobiology have been made over 
the past few decades. The development of new technologies and 
methods has provided researchers with a diverse set of tools to
study the brain. With the rise of highly parallelized computing, 
computational neuroscience has become one of the most important 
approaches to studying neuronal systems (\citet{trappenberg2009fundamentals}), 
offering new perspective on
brain function. It has enabled the simulation of large-scale neuronal 
networks, allowing us to analyze their behavior without relying solely
on vast amounts of real-world experimental data. As a result, we can 
now investigate brain systems in greater detail and gain a more precise 
understanding of their underlying principles.

With the rapid expansion of the machine learning, particularly deep neural 
network (DNN) models, neuroscientists have sought to apply these techniques
too. State-of-the-art convolutional DNN models have demonstrated outstanding
performance in task such as image classification and object detection 
(\citet{krizhevsky2012imagenet}, \citet{li2014medical}). 
These methods have also been used to model certain brain regions, often
yielding promising results. However, they come
with significant limitations (\citet{celeghin2023convolutional}).
Traditionally researchers using DNNs typically disregard the anatomical structure
and constraints of real neuronal networks. For instance, they rely exclusively on 
feed-forward architecture, whereas real brain networks are highly recurrent.
The models then typically prioritize task performance over biological plausibility
and interpretability.

On the other hand, usage of biologically plausible models such as 
spiking neural network (SNN) models have been a fundamental approach in computational
neuroscience (\citet{ghosh2009spiking}, \citet{yamazaki2022spiking}). 
These models attempt to bridge biological knowledge with computational
methods by incorporating biologically relevant constraints, preventing overfitting to
training datasets. However, SNNs face their own challenges, particularly the need 
to precise mathematical formulations to define networks behaviors, which can
limit their flexibility and scalability (\citet{izhikevich2004model}).

One of the most widely studied yet complex brain regions is the visual cortex.
Due to its intricate structure, studying it directly is almost impossible.
It makes the computational approaches essential for understanding it. 
Thanks to extensive experimental research the subregion called primary visual 
cortex (V1) is the best understood (\citet{miikkulainen2006computational}). 
Modern research on V1 typically employs 
either CNNs or SNNs, but both approaches have notable 
drawbacks (\citet{niell2021cortical}).

In our work, we try to unravel these drawbacks and combine both DNN models
with the biological constraints of the system to unravel the mystery of 
the V1. We use biologically constrained recurrent DNN model to
study the selected region of V1. We apply the following constraints:

\begin{itemize}
    \item We layer the architecture of the
    model using the known anatomical constraints. Building on that, the one 
    neuron in our network corresponds to some neuron in the real-life system. 
    This fact helps us in deciphering of the parameters of our model, and
    of our interest. This approach allows us to study the dynamics of the 
    visual cortex in a more realistic manner and provides us with 
    insights that are more aligned with the actual biological processes.
    \item We differentiate between excitatory and inhibitory neurons.
    \item We try to address the importance of the real-life neuron transfer
    function using small shared DNN modules across the defined anatomical layers. 
    These modules should mimic the complexity of the activation function 
    of the real-life neurons and stress the differences between them and 
    classical non-linearity functions used in DNNs, such as ReLU and tanh.
    They should also introduce some kind of memory of the neurons that might
    adapt to based on the previous stimuli.
    \item We present the synaptic adaptation modules. These modules should address
    the adaptability of the neuronal synapses to increased rates of the stimuli 
    from given neuronal layers.
\end{itemize}

By incorporating these anatomical constraints, we aim to improve 
the predictive power and interpretability of our models, ultimately 
contributing to a better understanding of the selected brain region.

In our research we have obtained the following results. 
\begin{itemize}
    \item Usage of the recurrent DNN model with biologically constraint 
    architecture without the DNN modules of the neurons captures the 
    mean neuronal responses reasonably well. Although, there is still
    room for improvement in terms of capturing the model dynamics.
    \item Usage of shared DNN modules of neurons significantly improved the
    model performance especially in terms of model dynamics.
    \item Differentiation in inputs from excitatory and inhibitory
    neurons to neuronal DNN module slightly improves the predictions in
    comparison to propagating the input signal without differentiation
    of the neuronal types.
    \item Usage of RNN model with LSTM cells of shared neuronal module
    slightly improved the performance of the model. This we address to
    introduction of neuron memory to the model.
    \item Usage of synaptic adaptation module does not improve the
    spatio-temporal resolution of the model in spite of introduction of 
    the synaptic memory. This might well be because of non-suitability of
    the tested hyperparameters.
\end{itemize}

The thesis is structured into several sections. In the first section
we introduce the reader to the theoretical background of the computational
neuroscience with the main focus on the visual processing and especially
to primary visual cortex (V1). Alongside with this, we introduce the reader
with the classical model that is spiking models and to selected intrinsical
aspects from the machine learning field. In the second section, we profoundly
describe our approach in the modeling of the system. We describe the architecture
of our model, dataset and evaluation metrics there. In the third section
we provide the experimental results of our study.



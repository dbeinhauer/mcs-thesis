\chapwithtoc{Introduction}

Significant advances in neurobiology have been made in recent decades. 
The development of new technologies and
methods has provided researchers with a diverse set of tools to
study the brain. With the rise of highly parallelized computing, 
computational neuroscience has become one of the most important
approaches to studying neuronal systems (\citet{trappenberg2009fundamentals}), 
offering a new perspective on
brain function. It has enabled the simulation of large-scale neuronal
networks, allowing us to analyze their behavior without relying solely
on real-world experimental data subject to various limitations. As a result, we can
now investigate brain systems in greater detail and gain a more precise
understanding of their underlying principles.

With the rapid expansion of machine learning, particularly deep neural
network (DNN) models, neuroscientists have also sought to apply these techniques.
State-of-the-art convolutional DNN models have demonstrated outstanding
performance in tasks such as image classification and object detection
(\citet{krizhevsky2012imagenet}, \citet{li2014medical}). 
These methods have also been used to model certain regions of the brain, often
yielding promising results. However, they come
with significant limitations (\citet{celeghin2023convolutional}).
Traditionally, researchers using DNNs typically disregard the anatomical structure
and constraints of real neuronal networks. For instance, they usually rely exclusively on feed-forward architecture, whereas real brain networks are highly recurrent. As a consequence these DNN models typically only predict the average response of neurons over a period of time and hence do not fully capture the complex non-linear dynamics of the biological neural networks. Ultimately, currently, ML approaches in neuroscience prioritize task performance over biological plausibility and, as a consequence, interpretability.

On the other hand, the usage of biologically plausible models such as
spiking neural network (SNN) models has been a fundamental approach in computational
neuroscience (\citet{ghosh2009spiking}, \citet{yamazaki2022spiking}). 
These models attempt to bridge biological knowledge with computational
methods by incorporating biologically relevant constraints, thus mechanistically explaining how the computations performed by the brain are implemented in the biological neural substrate. However, such SNN approaches face their own challenges, particularly limited ability to fit such models to data directly, therefore requiring a prior understanding of the system under study to generate precise mathematical formulations that define the behavior of the spiking network, limiting their flexibility and scalability (\citet{izhikevich2004model}).

One of the most studied, yet complex brain regions is the visual cortex.
Thanks to extensive experimental research, the cortical subregion, the first to process visual information that arrives from the retina, called the primary visual
cortex (V1), is the best understood (\citet{miikkulainen2006computational}). 
Extensive work on V1 involving both CNNs or SNNs have been undertaken, but both approaches have notable drawbacks (\citet{niell2021cortical}).

In this work, our objective is to overcome the limitations of both the CNN and SNN
approaches by integrating biological constraints into the design of recurrent neural network (RNN) models to better understand the primary visual cortex (V1). In this proof-of-concept thesis, we will rely on synthetic data generated by a SNN model of cat V1 developed by \citet{antolik2024comprehensive}, which we will use to train our novel RNN architectures. Specifically, we focus on predicting synthetic neuronal responses in layer IV (LIV) and layers II and III (LII/III) of V1, the first two visual cortical processing stages, using input from lateral geniculate nucleus (LGN) neurons. 

While in this proof-of-concept initial study we train our model on synthetic SNN data, our long-term goal is to achieve strong predictive performance on real V1
neuronal recordings as well. Our novel V1 modeling RNN architecture incorporates the following biological constraints overlooked in previous ML models of visual system:

\begin{description}
\item[Anatomical structure alignment:] The architecture of our
model is layered according to known anatomical constraints. Each
neuron in the network corresponds to a specific neuron in the reference SNN system. 
Such one-to-one correspondence between the reference SNN and RNN system facilitates
straightforward principled validation of the ability of the novel RNN architectures to approximate the full dynamical complexity of the SNN systems. In future application to real data, such one-to-one correspondence would be dropped with the 
observable biological neurons linked to only small subset of the full RNN network.

\item[Excitatory and inhibitory neuron differentiation:] We explicitly
distinguish between excitatory and inhibitory neurons, enforcing
biologically plausible behavior within the architecture and ensuring
that specific neuronal types function as expected.

\item[Biologically inspired activation functions:] Instead of standard
activation functions such as ReLU or tanh, we introduce small 
DNN or RNN module in place of each reference SNN neurons, that approximate the biological transfer function of the given neuron. The parameters of these transfer modules are shared across each given anatomically defined layer. These modules aim to approximate the complexity of biological neuronal transfer function that 
involves number of processes that cannot be accounted for by simple monotonic point non-linearities, such as adaptation. They allow us to capture these non-linearities and allow neurons to retain a form of memory that adapts to previous stimuli.

\item[Synaptic adaptation modules:] To model the plasticity of neuronal
synapses, we incorporate modules that adjust synaptic responses based on
increased stimuli rates from specific neuronal layers. This mechanism aims
to reflect the synaptic adaptation processes observed in biological
synapses.
\end{description}

By embedding these anatomical constraints into our model, we aim to
enhance both the predictive power and the interpretability of neural simulations,
ultimately contributing to a better understanding of the selected brain region.

In this thesis, we have obtained the following results. 
\begin{description}
    \item The novel biologically constrained RNN architectures without the DNN modules can be trained to capture well the mean neuronal responses. However, the reconstruction of the SNN dynamics is limited.
    \item The use of shared DNN modules of neurons significantly improved the performance of the model, especially in terms of capturing the dynamics of the SNN reference model.
    \item Differentiation in inputs from excitatory and inhibitory neurons to the neuronal DNN module slightly improves the predictions compared to propagating the input signal without differentiation of the neuronal types.
    \item The use of the RNN model with RNN cells of the shared neuronal module slightly improved the performance of the model.
    \item The use of the synaptic adaptation module does not improve the spatio-temporal dynamics of our model.
\end{description}

The thesis is structured into several sections. In the first sections,
we introduce the reader to the theoretical background of the field of neurobiology and computational
neuroscience with the main focus on visual processing and especially
the primary visual cortex (V1). Alongside this, we introduce the
reader to the modeling approaches and machine learning aspects
closely related to our study. In the following section, we
describe in depth our approach in modeling the system. We
describe the architecture of our model, dataset and evaluation
metrics there. In the end, we provide the experimental
results of our study and analysis of the results.
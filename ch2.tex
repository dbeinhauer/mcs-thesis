\chapter{Computational Neuroscience}
\label{chap:computation_neuroscience}
Computational neuroscience is an interdisciplinary field that
utilizes mathematics and computer science knowledge to better
understand the nervous system and brain. With the rampant
ascent of the computational efficiency and technology for
obtaining more data from the experiments this field
became more and more relevant. Especially, in the last
years with the ascent of the DNNs. The models are great due for
its replicability, stability. It does not contain any noise and so on.

In computational neuroscience we commonly define three categories of models
\citet{dayan2005theoretical}. These models are \emph{descriptive} that summarize
large number of experimental data and describe what the system does. There models
might also be biologically plausible but its main goal is to describe the 
system, not to explain it. The example of it might be some mathematical description
of the behavior of the specific aspect of the brain like receptive field of a neuron 
or spiking rate of the neuron.
The second type is \emph{mechanistic} model, these 
models aim to explain how the system works on the anatomical level. The last
type is \emph{interpretive} model, these models use information and computational
principles to explore the cognitive and behavioral functions of the system, and they 
try to address why the system operates as it does. Overall,
it is typically not clear which model suits the problem the best and often the models
itself overlap the categories. This thesis might be a great example of the overlap of
the categories, where we try to accurately
describe the behavior, alongside with this we try to explain the behavior as we 
focus on the model explainability. As a result, we can say our model kind of overlaps
all of the categories.

In the following sections, we will describe the most common terms in the computational
neuroscience fields, typical evaluation approaches and we will list the selected 
modeling approaches that are used in the field.

\section{Introduction to System Identification in Visual System}
\label{sec:system_identification}
Since our thesis is focused on the study of the visual system we will focus mainly on
introducing the aspects closely related to the study of visual system. It is
worth noting though that the primary visual cortex and visual system overall is one of
the most studied ones in the computational neuroscience and thus a wide range of the
terms has been introduced and defined on the example of the visual system (TODO: cite
Abott paper). 

Characterizing the relationship between stimulus and neuronal response is a complex
task since it does not only depend on the stimulus itself but also on the other 
factors such as temporal state of the neuron, temporal separation between the stimuli
and many others. Because of that the neuronal responses might differ in trials on 
the same stimuli.This makes accurate prediction of the single neuronal spike almost 
impossible. Instead, we try to capture the probability distribution and perform well 
across all the trials. Typically, many neurons respond to a given stimuli, so, 
the properties of the stimulus are encoded in the large population of neurons and thus 
it is arguable more important to understand and properly describe the whole 
population of neurons rather than just perform well on the single neuron predictions.

The nice overview of the system identification can be seen in 
\citet{annurev:/content/journals/10.1146/annurev-vision-091718-014731} which served as
a source of information for the following sections.

\subsection{System Identification}
\label{subsec:system_identification}
The task of the system identification is to estimate and describe the unknown system 
based on the measured data 
\citet{annurev:/content/journals/10.1146/annurev-vision-091718-014731}. 
In the context of the visual system, the system is the
corresponding neuronal population and the measured data consists typically of the visual 
stimuli and the corresponding neuronal responses. In the typical approach we 
separate the data to input data and output data. Overall we can define the task of 
system identification as the following:

\begin{equation}
    r = f(s)
    \label{eq:system_identification}
\end{equation}

Where $s$ is the stimulus, $r$ is the response of the system typically in form of 
spikes or spike trains on the stimulus and 
$f$ is the unknown mapping function that describes the system. Our goal is to 
estimate the function $f$.

\subsection{Stimulus}
\label{subsec:stimulus}
The stimulus can be any input signal that can invoke the response. Typically it is 
represented as a vector or a matrix or a sequence of them of the appropriate values. 
In the context of the visual system, the stimulus is typically the image or the 
sequence of images (or video) that are typically represented as a matrix.

The general problem with the stimuli is what kind of the stimuli is appropriate 
for the given task \citet{Carandini10577}. The typically used stimuli were the 
simple arbitrary stimuli like bars or sinusoidal grating patterns for example in 
\citet{hubel1965receptive}'s work. However, these
stimuli are not very informative as they do not really occur in the real world.

Another widely used method is the white-noise stimuli \citet{dayan2005theoretical},
\citet{chichilnisky2001simple}. The idea under this stimuli is 
that each pixel of the image stimulus should be independent to each other. This 
if defined correctly should cover uniformly the whole space of the possible visual 
stimuli. Although it has been proven that this stimuli is to some extend very 
informative there are also several drawbacks related to it \citet{Talebi1560}. 
Mainly the thing that since neurons evolved to primary process the natural stimuli, 
the white-noise stimuli does not affect especially neurons highly specialized to 
some aspect of the natural stimuli enough. 

This is the reason why many of the recent 
researches focus on the natural stimuli \citet{sonkusare2019naturalistic}, 
\citet{lurz2020generalization}, \citet{antolik2024comprehensive}. Using this we can 
encounter alongside with other properties of the neuronal receptive fields also the 
selectivity of the neurons to typical properties of the natural stimuli.

\subsection{Spike Trains}
\label{subsec:spike_trains}
Neurons propagate information through action potentials often called \emph{spikes}. 
Although, it is typical that they differ in variety of properties like duration, 
amplitude and shape, we typically 
treat them as binary identical events. Furthermore since the spike duration
is typically very short (around 1 ms), we treat them as instantenous events. We then
typically treat action potentials as a sequence of binary events in time, so called
\emph{spike trains}. As mentioned before, the spike trains typically differ quite 
a lot between the trials, so we typically describe them using a statistical metric 
called \emph{firing rate}. Based on \citet{dayan2005theoretical} is this term defined
for various properties, so they call it \emph{spike-count rate}. In our thesis, we 
will stick with the term firing rate, since we won't focus on other properties 
that are marked as firing rates. The definition is basically the average number of 
spikes during the trial:

$$r = \frac{n}{T} = \int_{0}^{T}d\tau \rho(\tau)$$

Where $r$ is the firing rate, the first equation represents the definition of 
firing rate with given time step (typically 1 ms). In this equation $n$ is the 
number of spikes during the trial, $T$ is the duration of the trial (number of 
time steps), the second equation represents the same property but in the continuous
time interval. It is worth noting that also \emph{time dependent firing rate} can 
be defined, that is defined as firing rate in the given time interval. However, this
metric is typically used as an average over multiple trials since with the compressing
time the noisiness of the data cause by spiking variability grows. If we 
select the time interval to be very small (e.g. 1 ms), we work with high temporal 
resolution spiking rates and since we know the average duration of the single spike is 1ms, we 
can treat their values in one trial as binary values. And if we compute their average
over the trials, we get the probability of the spike in the given time step. In the 
typical usage there is a trade-off between temporal resolution and the noisiness of the
data that steeply grows with the resolution. Another aspect is the computational 
complexity since the higher resolution requires more data to be processed to reduce 
the noise. In the real-life examples, there is typically selected the time 
resolution that it the best trade-off between those two aspects. For example in our
thesis although we aim to have a good temporal resolution, we selected the discrete 
time bin 20 ms.

\subsection{Mapping function}
\label{subsec:mapping_function}
The last but not least part of the system identification problem is the mapping 
function $f$. During the studies of the neural systems the most common 
approach how to describe the behavior of the system is to use some mathematical 
representation \citet{annurev:/content/journals/10.1146/annurev-vision-091718-014731}.
There are many different approaches how to model the mapping function. Based on the 
\citet{annurev:/content/journals/10.1146/annurev-vision-091718-014731} there are 
some main factors that needs to be considered when selecting the model. The 
first factor is how easy it is to optimize the parameters, second one is how well does
the model generalize to the unseen data and the third one is what insights of the
system properties does the model provide.

These aspects are highly affected be the selected experimental data. The researcher 
needs to consider what the amount, quality and type of the stimulus data to suit 
the best the selected model. The question that we have briefly described in the section 
\ref{subsec:stimulus}.

\subsubsection{Maximum a Posteriori Estimation}
\label{subsubsec:map_estimation}
In the ideal case, statistical models would be designed simply based on the 
experimental recordings. In the real life situation the task is typically connected
to the estimation of the model parameters. The choice of the mathematical form of the 
model and its parameters is often the most crucial part of the model design. 

The most common approach in parameter selection optimization in neuroscience is the 
\emph{maximum a posteriori estimation} (MAP) 
(\citet{wu2006complete}, 
\citet{annurev:/content/journals/10.1146/annurev-vision-091718-014731}). This approach
in comparison to the kind of classical Maximum Likelihood Estimation (MLE) 
\citet{alpaydin2020introduction} incorporates additional model constraints alongside
with the likelihood of the data. The property that is useful in the neuroscience 
field where the spiking neuronal data are often too noisy to only rely on them.
On the other hand we can apply prior knowledge about the system to enhance our model
parameters estimation. 

Let us denote model parameters as $\Theta$, we assume we have pairs of input-output 
observed data, in other words that we have pairs $(s, r)$, where $s \in S$ is the 
stimulus and $r \in R$ is its appropriate response where both $S$ and $R$ consists of
$N$ examples. These hold the relationship
defined in the equation \ref{eq:system_identification}. The MAP framework tries 
to maximize the posterior probability distribution $p(\cdot|\cdot)$ of the model 
parameters as follows:

\begin{align*}
    \Theta_{MAP} &= \arg\max_{\Theta} p(\Theta | S, R) \\
    &= \arg\max_{\Theta} p(R | \Theta, S) \cdot p(\Theta) \\
    &= \arg\max_{\Theta} \left[\prod_{i=1}^{N} p(r_i | f_{\Theta}(s_i)) \cdot p(\Theta)\right] \\
    &= \arg\min_{\Theta} 
    \left[
    \underbrace{-\sum_{i=1}^{N} 
        \overbrace{\log p(r_i | f_{\Theta}(s_i))}^{\text{Likelihood}}
    }_{\substack{\text{Negative Log Likelihood} \\ \text{(Loss function)}}}
    - \underbrace{
        \overbrace{\log p(\Theta)}^{\text{Prior}}
    }_{\substack{\text{Negative-log prior} \\ \text{(Regulatizer)}}}
    \right] \numberthis \\
\end{align*}
\label{eq:map_estimation}

The derivation of the equation \ref{eq:map_estimation} is based on the basic probability
rules. For comprehensive explanation please refer to \citet{alpaydin2020introduction}, 
\citet{wu2006complete}, \citet{annurev:/content/journals/10.1146/annurev-vision-091718-014731}.
It is work noting that in the terms of our task, the probability term $p$ refers to 
noise distribution of our data, the term $f_{\Theta}$ is mapping function (our model) 
with the parameters $\Theta$ and finally the term $p(\Theta)$ refers to prior knowledge 
that we have about the system we want to model. The prior term is the one distinguishing
the MAP from teh MLE. The reason why we use typically prioritize this approach is that in
case we do not have any prior knowledge about the system, the noisiness of our data 
might easily led our model to overfitting as the number of experimental data is typically
quite low and the likelihoods of different models is then rather similar. In other words,
the first term of the resulting equation is the loss function that tells us how well 
does our model fit the data and the second term is the regularization term that tells 
us how complex is our model. The MAP frameworks then tries to find the optimal 
balance between the two terms.


\section{Modeling Approaches}
\label{sec:modeling_approaches}
One of the most important aspects of the computational neuroscience is the selection
of the appropriate model. More complex models might capture the behavior of the system
better but they are also more prone to overfitting (as briefly mentioned in 
\ref{subsec:mapping_function}). In this section we will focus on the three bearing 
modeling approaches that has been used in the computational neuroscience field. We 
will start with the most simplistic models that use simple mathematical methods to easily fit the model. We will call them as \emph{classical approach models},
then we will focus on the models using DNNs and finally we will describe the 
models that tries to reflect the biological properties of the neurons called 
spiking neural networks (SNNs). The main source of information for this section was the paper from \citet{annurev:/content/journals/10.1146/annurev-vision-091718-014731} that outlines the course of the data driven approaches in study of the visual neural system. We thus encourage the reader to study the mentioned paper in necessity for deeper explanation of the techniques mentioned below.

\subsection{The Classical Approach Models}
\label{subsec:classical_approach}
At the beginning of the field, the models were usually
simplistic and the main goal was to describe the behavior of the system using the models
that are easy to fit. The simplest approach is the so-called \emph{linear model}. In these we simply define the response of a neuron on a multiple stimuli using the weighted sum:

$$r_{\text{L}} = f_{\text{linear}}(\boldsymbol{s}) = \boldsymbol{w}\boldsymbol{s} = \sum_{i} w_i s_i$$

Where $r_{\text{L}}$ is the neuron response, $f_{\text{linear}}$ is the mapping function, 
$\boldsymbol{s}$ is the vector of all relevant stimuli and $\boldsymbol{w}$ is the
vector of their corresponding weights (also named as \emph{filter}).

This model can be easily extended using a non-linearity function to the 
so-called \emph{linear-nonlinear model} (LN). The model definition is then:

$$r_{\text{LN}} = g_{\text{LN}}(r_{\text{L}}) = g_{\text{LN}}(f_{\text{linear}}(\boldsymbol{s}))$$

Where $g_{\text{LN}}$ is the nonlinear function and $r_{\text{LN}}$ is the result response of linear-nonlinear model.

Until recent years the LN filter usage prevailed in the sensory neuroscience. There are several results for this. First, they offer straightforward interpretation of the neuronal responses in both computational (\citet{hubel1965receptive}, \citet{movshon1978receptive}) and biological aspect (\citet{mohanty2012membrane}, \citet{SHAPLEY2009907}, \citet{poirazi2003pyramidal}). To our concern the this approach showed a good prediction for a heavily studied neurons of the early visual system (\citet{SHAPLEY2009907}, \citet{baccus2002fast}, \citet{Carandini10577}). However, these techniques has been replaced by the more complex models in the recent years that available studies of the visual pathway in more profound manner and in larger scale (\citet{Maheswaranathan340943}, \citet{Butts11313}, \citet{keat2001predicting}).

\subsection{Deep Neural Network Approach}
\label{subsec:deep_learning_approach}
While it has been proven that multiple cascades of the LN units \ref{subsec:classical_approach} can in principle capture quite complex behavior (\citet{cybenko1989approximation}, \citet{HORNIK1991251}), it has been shown that DNN techniques outperform the classical techniques (\citet{MAL-006}, \citet{Kriegeskorte2015dnn}). Though these techniques are mathematically more complex and computationally more intensive, the rampant ascend of the computational power and new automated tools such as TensorFlow (\citet{TensorFlow}) and Pytorch (\citet{paszke2017automatic}) made this techniques suitable for a wide variety of data (\citet{lecun2015deep}). As a result these techniques have proven to outperform classical approach models in various visual system identification tasks such as retina (\citet{Maheswaranathan340943}), V1 (\citet{cadena2019conv}, \citet{kindel2017usingdeeplearningreveal}) and even the extrastriate visual cortex regions (\citet{zareh2024deep}). Majority of the state-of-the-art approaches are utilizing the \emph{convolution neural networks} (CNNs) (\citet{NIPS2012_c399862d}) that are proven to perform greatly in the visual tasks and also outperformed the classical approaches (\citet{zhang2019convolutional}, \citet{cadena2019conv}).

On the other hand, there are also limitations of this approach. The clearly problematic limitation is the amount of data DNN models require to properly fit the model to a single neurons as the amount of biological experimental data that we can obtain such neuronal recordings is still a limitating factor in the field (\citet{zhang2019convolutional}), some of the models has partially solved this problem while utilizing the local constraints of the receptive fields and thus fitting multiple neurons simultaneously (\citet{antolik2016local}). 


\subsubsection{Recurrent Neural Networks}
\label{subsubsec:rnns}
Another drawback connected with majority of the currently state-of-the-art DNN models is that they are typically using \emph{feed-forward} architecture meaning that the stimulus is processed sequentially which contradicts with the biological system where the signal is processed in also in the recurrent (or feed-back) way. While these models are able to model complex functions and even perform well in our task it is clear from the visual system that in order to enhance our biological explainability and overall quality of the model it would be needed to add the recurrent connectivity (\citet{Kafaligonul2015rnn}, {shou2010functional}, \citet{kar2019evidence}). This fact mainly affects the model in capabilities to adjust the signal based on the recent state and thus limits the model mainly in description of the temporal dependencies (\citet{shou2010functional}).

The possible solution of this problem might be usage of \emph{recurrent neural networks} (RNNs) (\citet{medsker2001recurrent}) that are DNNs that contains apart from feed-forward connections also the recurrent ones. It has been proven that RNNs are great for capturing time-dependent properties in several neural systems (\citet{mante2013context}, \citet{song2016excinhrec}). Another advantage this approach provides is that each neuron can fulfill wide variety of biological constraints and thus it provides us the framework to model individual neurons (\citet{mante2013context}, \citet{masse2019circuit}, \citet{kim2019rnnframework}). With the advances in the computational power and DNN frameworks as mentioned in \ref{subsec:deep_learning_approach} there has been a several studies using complex networks modeling different brain regions focusing on wide variety of tasks from motor control (\citet{sussillo2015neural}, \citet{saxena2022motor}) to cognitive functions (\citet{masse2019circuit}, \citet{goudar2023schema}). Despite the advantages in RNN applications in neural processing models, the application of RNN in visual processing task still remain problematic and is overshadowed by the CNN models \ref{subsec:deep_learning_approach}. Up-to-date only a few studies proposed a RNN architecture comparable to CNN like for example the study by \citet{NEURIPS2024_f536d569}.

\subsubsection{Interpretability of DNNs}
\label{subsubsec:interpretability_dnn}
In the subsection \ref{subsec:deep_learning_approach} we have already brought a few times the problem related to the interpretability of the DNN models. There are critics of the usage of DNNs in the system identification task that argue that this approach just replace the simpler non-interpretable approach with complex one and does not bring any new insight into the understanding the system. In the subsequent text we will try to address and present some argument in favor of the DNN model. For further insight please refer to a paper from \citet{Kriegeskorte2015dnn} that served as a main source of information for this section.

First, it is worth to note that it is true that if our model performs well in prediction of the neuronal responses it does not provide us the insight into the higher levels of the information processing and we cannot easily interpret how the network translates the information to some higher level description. However, since the model capture the low level dynamics it provides us the framework to study the biological system in silico and thus better deal with the low amount and noisiness of the in vivo experimental results.

Second typical concern is about the complexity of the models. The critics often say that the DNNs often provides such a level of complexity that its behavior cannot be easily understood. To response to this might be that since the intelligence and neural computing is very intrinsic and complex it just might not be possible to describe this behavior using some simple intuitive mathematical description. In other words, we should try to focus on building as simple models as possible but these models should be able to capture the dynamics.

With these points we might now look at the question of analogy between the DNNs and biological networks. First, it is worth noting that some level of abstraction is desirable as the objective of the modeling is not creating identical system, but rather to introduce the right level of abstraction to describe the system. We thus should properly define what properties of the system should our model capture and measure the quality of the model relevant to the selected aspect of the system. There are for sure a lot of challenges and drawback of the DNN approach such as its typical feed-forward architecture and thus lack of biological plausibility. In order to improve the interpretability, it makes sense to try to focus on more plausible approaches like usage of RNNs mentioned in the \ref{subsubsec:rnns}. With this approach we can then enhance our models of visual, motor or cognitive functions using DNNs models. To sum things up, usage of DNN models might definitely be beneficial for studies in computational neuroscience and one should not be demotivated by its seamless black-box character.

\subsection{Spiking Neural Networks}
\label{subsec:spiking_neural_nets}
Until now we have focused mainly on the predictive power in the visual system recognition task and does not really focus on the biological relevance of the model with a few exceptions where we mentioned this phenomenon mainly in part focusing on the RNNs in subsection \ref{subsubsec:rnns}. Now we will introduce different modeling approach the \emph{spiking neural networks} (SNNs) that incorporates the focuses on the biological relevance and tries to create the model that corresponds to our knowledge of the system. The SNNs are alongside with CNNs (\ref{subsec:deep_learning_approach}) the state-of-the-art methods in visual system recognition.

In the subsequent text we will introduce one of the base approaches in SNNs called \emph{Integrate-and-Fire Models} that is one of the most used SNN models of the neural networks including the model from \citet{antolik2024comprehensive} that serves as our source of artificial dataset and whole behavior we aim to mirror in our study. Since these models aims to reflect the biological concepts it is worth to have an insight to those too, we have described the key biological aspects in the chapter \ref{chap:visual_system}. Especially the section on neuron \ref{sec:neuron} is the most relevant in our case. It is also worth knowing the key aspects of modeling in computational neuroscience that can be found \ref{sec:system_identification}. For the further understanding reader can refer to one fo the classical neuroscience books \citet{bear2020neuroscience} or for computational neuroscience basics to \citet{dayan2005theoretical}. For further understanding of the SNN models reader can refer to a book from \citet{gerstner2002spiking}.

\subsubsection{Integrate-and-Fire Models}
\label{subsubsec:integrate_and_fire}
There is a wide variety of the models that try to capture the biological aspects of the neural network. Apart from Integrate-and-Fire models it might be the Hodgin-Huxley model (\citet{hodgkin1952quantitative}). This model is tries to capture the system properties in depth and provides high level of accuracy in terms of reproducing the electrophysiological properties. However, because of its high complexity those are often hard to analyze and model. To deal with this problem there has been introduced SNN models. These typically focus on the modeling binary spikes (\ref{subsec:spike_trains}) while the membrane potential (\ref{fig:action_potential}) reached the given threshold.

The example of such models can be family of integrate-and-fire models. In such a model the action potential is described binary as either no spike or a spike, no additional information about the spike shape, duration etc. The only focus is on prediction of the exact time when the spike happens. In these models we focus on the modeling of the membrane potential. We define $U_{\text{threshold}}$ the capacity needed to elicit the action potential and $U_{\text{rest}}$ the capacity to which the membrane capacity resets after the spike (resting membrane potential \ref{subsec:action_potential}). The model of a neuron is then simplified to an electric circuit representation with capacitor and resistor are in parallel. In the following text we will briefly describe the model for deeper description please refer to \citet{dayan2005theoretical} and \citet{gerstner2002spiking} that served as a source of information for the following text.

The idea is based on analogy between the neuron and electric circuit and basic physical properties of the circuit. When we inject the electric current $i(t)$ in time $t$ to the neuron (either experimentally or the neuron elicit the action potential \ref{subsec:action_potential}) the charge needs to go somewhere and thus it will charge the neuron membrane. This membrane then represent the role of a capacitor with capacity $C$. The membrane thus is not perfectly isolated and some of the charge leak outside, this can be represented as a constant leakage membrane resistance $R_m$. We then can simplify the membrane as an electric circuit with capacitor and resistor connected in parallel (TODO: maybe picture). With using the basic physics properties and mathematical simplifications we get the differential equation describing the standard mathematical form of the model as:

\begin{equation}
    \tau_{m} \odv{u}{t} = -u(t) + R_{m}i(t)
\end{equation}
\label{eq:integrate_and_fire}

Where $\tau_{m}$ is the membrane time constant, $u(t)$ is the membrane potential in time $t$.

In the integrate-and-fire models the spikes are not defined explicitly. They are represented as discrete events that happen while the threshold potential $U_{\text{threshold}}$ is reached. The firing time $t^{(f)}$ is defined as:

\begin{equation}
    t^{(f)}: \quad u(t^{(f)}) = U_{\text{threshold}}
\end{equation}
\label{eq:firing_time}

In our model we do not describe the spike explicitly, rather we reset its potential immediately after the threshold potential is reached to resting membrane potential as:

\begin{equation}
    \lim_{\delta \to 0; \delta > 0} u\left( t^{(f)} + \delta \right) = U_{\text{rest}}
\end{equation}
\label{eq:resting_potential}

For $t > t^{(f)}$ the dynamics is defined by the same equations from the start. The result of the model is then sequence of firing times for each neuron $i$ as $t^{(f)}_i$ where $f = 1, 2, \dots$ is the count of the spike.

This concept describes the model called \emph{leaky integrate-and-fire model} that is the basic variant of integrate-and-fire model. Apart from this there is a wide variety of the model variant. If the reader is interested he can refer to the classical books concerning SNN from \citet{dayan2005theoretical} and \citet{gerstner2002spiking}.

\section{Evaluation Methods}
\label{sec:evaluation_methods}
Until now we have described the basic concepts of the computational neuroscience and listed a few of the commonly used techniques. However, we have not mentioned how we evaluate the performance of the models.
\chapter{Methods}
\label{chap:methods}

In this chapter, we begin by describing the artificial stimulus dataset used to train our model, including a brief overview of the model from which the dataset originates. We then detail the preprocessing steps applied to the dataset to make it compatible with our model. The final and major section of this chapter focuses on our biologically plausible recurrent neural network (RNN) model of the primary visual cortex (V1).

\section{Spiking Model of Cat Primary Visual Cortex}
\label{sec:cats_model}
A major challenge in computational neuroscience, particularly in system identification tasks \ref{sec:system_identification}, is the limited availability and quality of biological data. This issue becomes even more pronounced when developing models using deep neural networks (DNNs), as discussed in Section \ref{subsec:deep_learning_approach}. The noise and expense of biological data acquisition motivate the use of artificial datasets. In our work, we utilize data generated by a state-of-the-art spiking model of the cat V1 developed by \citet{antolik2024comprehensive}.

Originally, we intended to fine-tune the model using real data from multielectrode array recordings in macaque V1, spanning tens of thousands of images. However, due to time constraints, this was not feasible within the scope of this thesis. Integration of real data remains a promising direction for future research.

\subsection{Spiking Model Description}
\label{subsec:spiking_model_description}

The artificial dataset is derived from a biologically detailed spiking model of cat V1 introduced by \citet{antolik2024comprehensive}. Designed to simulate visual experiments, this model aims to address the challenges inherent in biological data acquisition.

The model is implemented as a spiking neural network (SNN) using the exponential integrate-and-fire neuron model (\citet{FourcaudTrocm11628}), an extension of the leaky integrate-and-fire model (see Section \ref{subsec:spiking_neural_nets}). It captures the spatial layout of cortical layers IV and II/III in a 5.0 x 5.0 mm patch corresponding to the \emph{area centralis} of the retina, responsible for high-acuity vision (see Section \ref{subsec:retina}).

The model incorporates various biological properties, such as differentiation between excitatory and inhibitory neurons, synaptic depression, and local connectivity among neurons with similar orientation preferences (Section \ref{subsec:receptive_field}). It has been validated against a wide range of experimental data, including responses to white-noise and natural stimuli (Section \ref{subsec:stimulus}). Compared to other models, it is distinguished by its comprehensive inclusion of biological features (\citet{antolik2024comprehensive}).

A particularly notable strength of the model is its ability to reproduce spontaneous neuronal activity without artificial noise sources. This enables it to replicate trial-to-trial variability in stimulus responses (Section \ref{sec:evaluation_methods}), as demonstrated in \citet{baudot_animation_2013}. These capabilities were validated using blank gray stimuli (\citet{PAPAIOANNOU1972558}), making the model a valuable source of biologically plausible data.

Despite its strengths, the model remains a simplification of the biological system. For instance, feedback from cortical layers V and VI and the diversity of neuron types within layers are not explicitly modeled (Section \ref{sec:v1}). Furthermore, the recurrent pathway from layer II/III to IV serves as a proxy for these excluded modulatory circuits.

\subsubsection{Model Architecture}
\label{subsubsec:spiking_cat_architecture}
The model is substantially downsampled relative to the biological system. It comprises 108,150 neurons and approximately 155 million synapses, representing roughly 10\% of the neuronal density in cat V1 (\citet{beaulie1989number}). Neurons are distributed across layers according to biological data, with excitatory and inhibitory neurons present in a 4:1 ratio (\citet{bealuliee1992quantitative, markram_interneurons_2004}). The model includes both feedforward and recurrent connections within and between layers.

It also incorporates a simplified lateral geniculate nucleus (LGN) model (Section \ref{sec:lgn}), with distinct ON and OFF pathways (Figures \ref{fig:early_vis_processing} and \ref{fig:on_off_cells}). These 7,200 LGN neurons are spatially aligned with the receptive fields of their V1 counterparts. For further details, see \citet{antolik2024comprehensive}.

\section{Artificial Dataset}
\label{sec:artificial_dataset}
Our dataset consists of spiking activity generated by the model described above \ref{sec:cats_model}. It includes responses from 14,400 LGN neurons and 108,150 V1 neurons, recorded at 1 ms resolution. The data is organized into sequences corresponding to multiple experiments, each comprising alternating blank and natural image stimuli. Due to the model's comprehensiveness, unnecessary information was removed during preprocessing, as described below.

\subsection{Stimulus Sequence Structure}
\label{subsec:stimulus_sequence}
The dataset is divided into \emph{stimulus sequences}, each comprising alternating intervals of a blank gray screen and a natural stimulus image neuronal responses. Each pair forms a \emph{stimulus pair}, where the blank interval precedes the stimulus to ensure that only spontaneous activity is present before the image is shown.

Each stimulus sequence begins with a blank interval of duration $t_{\text{blank}}$, followed by a stimulus interval of duration $t_{\text{stim}}$. These pairs are repeated periodically. Blank intervals serve to reset neural activity, preventing carryover effects from previous stimuli. Notably, the final blank interval following the last stimulus is omitted, introducing a minor inconsistency.

\subsection{Experiment Definition}
\label{subsec:experiment}
To facilitate processing and reduce memory demands, stimulus sequences are divided into smaller units called \emph{experiments}. Each experiment consists of the second half of a blank interval, the corresponding stimulus interval, and the first half of the following blank interval. This ensures that the neural activity at the boundaries of the stimulus reflects spontaneous states.

\begin{defn}[Experiment]
    Let $S$ be a stimulus sequence consisting of $N \in \mathbb{N}$ pairs of blank and stimulus intervals $(b_i, s_i)$. Let $l_b$ and $l_s$ be the lengths of the blank and stimulus intervals, respectively. Then, for $j \in [1, 2, \dots, N-1]$, an experiment is defined as:

    $$
    \left(b_j\left[\frac{l_b}{2}:l_b\right]; s_j; b_{j+1}\left[0:\frac{l_b}{2}\right]\right)
    $$

    The final experiment omits the trailing blank interval:

    $$
    \left(b_N\left[\frac{l_b}{2}:l_b\right]; s_N\right)
    $$
\end{defn}
\label{def:experiment}

To maintain consistency, the first half of the initial blank interval is omitted. While this results in minimal data loss, it facilitates uniform formatting. Experiments lacking a post-stimulus blank interval are padded with zeros.

\subsection{Dataset Preprocessing}
\label{subsec:dataset_preprocess}
The original dataset includes detailed simulation data, much of which is irrelevant for training. We extract only the spike trains (Section \ref{subsec:spike_trains}) and segment them into experiments, reducing memory and computation demands significantly.

To streamline this process, we have developed a dedicated data processing pipeline that converts raw simulation output into the final experiment-based format used by our model. This pipeline is modular and reusable, allowing efficient processing of future datasets generated by the same spiking model. As such, it facilitates further experiments and model iterations without redundant implementation effort.

\subsection{Dataset Description}
\label{subsec:dataset_description}

After the raw dataset generated from the model by \citet{antolik2024comprehensive} undergoes the preprocessing steps described in Section \ref{subsec:dataset_preprocess}, we obtain the final dataset used in our model. The basic unit of this dataset is the experiment (Section \ref{subsec:experiment}). Each experiment is represented by six matrices corresponding to different neuronal populations. Two matrices represent the spike trains of ON and OFF LGN neurons (denoted as $s_{ON}$ and $s_{OFF}$), while the remaining four correspond to layer IV and layer II/III neurons, further divided into excitatory and inhibitory types: $r_{E_{4}}$, $r_{I_{4}}$, $r_{E_{2/3}}$, and $r_{I_{2/3}}$. The number of neurons in each population is defined in Section \ref{subsubsec:spiking_cat_architecture}, matching the configuration in \citet{antolik2024comprehensive}.

Each matrix has the format $\mathbb{R}^{N \times M \times T}$, where $N$ is the number of experiments, $M$ is the number of neurons in the given population, and $T$ is the number of time steps in each experiment (see Section \ref{subsec:experiment} for experiment definition).

In our artificial dataset, the blank stage lasts for 150 ms, and the natural stimulus stage lasts for 560 ms, both recorded in 1 ms bins. Thus, the spike train for a single neuron in an experiment spans 710 ms, composed of 75 ms of a blank interval before the stimulus, 560 ms of stimulus presentation, and 75 ms of the subsequent blank interval. The spike train is encoded in binary format: a 1 indicates a spike at a given time step, and a 0 indicates no spike. For some experiments at the end of a stimulus sequence, the final blank interval is missing. To ensure uniform experiment duration, we pad these with zeros (no spikes).

Although this padding does not accurately reflect biological spontaneous activity (which is non-zero, see Section \ref{subsec:spiking_model_description}), it affects only a minority of cases. We chose this pragmatic solution for its simplicity and minimal impact on model performance, avoiding additional complexity and computational overhead.

In classical machine learning, datasets are typically split into training, validation, and test subsets, often with randomized sampling. However, our use case is complicated by trial-to-trial variability in neuronal responses, necessitating multiple repeated trials in the validation and test datasets (see Section \ref{sec:evaluation_methods}). Because this variability arises mainly from spontaneous neural activity (\citet{antolik2024comprehensive}), we chose not to include multiple trials in the training set. Instead, we prioritized generating a wider variety of single-trial responses to diverse natural stimuli. This design choice balances several practical factors: dataset size, memory efficiency, training speed, and simulation time.

As a result, we do not apply randomized splits across the dataset but instead rely on a structured division between training and evaluation datasets.

\subsubsection{Train Dataset}
\label{subsubsec:train_dataset}

The training dataset consists of 500 stimulus sequences, each containing 100 pairs of blank and natural stimuli, all presented in a single trial. After preprocessing and experiment segmentation (Section \ref{subsec:experiment}), this yields a total of 50,000 unique experiments for training.

\subsubsection{Test and Validation Dataset}
\label{subsubsec:test_dataset}

The validation and test datasets comprise 18 stimulus sequences, each with 50 stimulus pairs repeated over 20 trials. After preprocessing, this results in 900 experiments, each with 20 trial repetitions. We randomly divide this set into validation and test subsets using a 1:9 ratio.

\subsubsection{Neuron Subset Selection}
\label{subsubsec:subset_selection}

To build a biologically plausible RNN model that mirrors the structure in \citet{antolik2024comprehensive}, one would ideally create a one-to-one mapping between model neurons and biological neurons. However, given the lack of structural constraints (e.g., known synaptic weights) and the impracticality of modeling over 100,000 neurons, we adopt an all-to-all connectivity scheme with neuron subsampling.

We randomly select 10\% of neurons from each population to reduce model complexity and memory consumption, making the model more tractable during training and fine-tuning. To mitigate sampling bias, we run experiments using 20 different random neuron subsets. Empirical testing has shown that using this 10\% subset produces performance metrics similar to those obtained with larger neuron subsets. Further evaluation of this design choice is presented in the next chapter.

\section{Model Description}
\label{sec:model_description}

In this section we will focus on the definition of the ground model and then on the biologically inspired extensions that we consequently applied to improve the model performance.

The task of the model is the visual system identification as described in Section \ref{sec:system_identification}. More profoundly our aim is design the RNN model that based on the input sequence that consists of the LGN neurons (\ref{sec:lgn}) responses from our artificial dataset described in the previous Section \ref{sec:artificial_dataset} predicts the appropriate responses of the selected V1 neurons whose responses we have in our dataset as mentioned in Section \ref{sec:artificial_dataset}. In terms of the visual system identification we have in each time step $t$ of the input experiment sequence \ref{subsec:experiment} a vector of neuronal responses from the LGN ON and OFF cells that serves as an input stimuli. Let's denote $s(t)$ as our vector of input stimuli (using notation of appropriate neuronal responses used in the Section \ref{sec:artificial_dataset}) in time step $t$: 

\begin{equation*}
    s(t)=\left(s_{ON}(t); s_{OFF}(t)\right)
\end{equation*}

Our task is to find the model $f$ that is a biologically plausible RNN that predicts the neuronal responses from the V1 neuronal responses on the stimulus in time $t$ from $s(t)$. Let's denote this as:

\begin{equation*}
    \hat{r}(t) = f(s(t))
\end{equation*}

Where $\hat{r}(t)$ consists of:

\begin{equation*}
    \hat{r}(t) = \left(\hat{r}_{E_4}(t); \hat{r}_{I_4}(t); \hat{r}_{E_{2/3}}(t); \hat{r}_{I_{2/3}}(t)\right)
\end{equation*}

Also note that we will mark the predictions using the hat $\hat{.}$ symbol and without this symbol we will mark the targets from our artificial dataset.

The aim of the task is to to minimize the difference between predictions $\hat{r}(t)$ and targets $r(t)$ and to accurately capture the spatiotemporal dynamics of the single neuronal responses. As mentioned in Section \ref{sec:evaluation_methods}, there is currently no consensus in the ideal evaluation metrics, thus we will use Pearson's correlation coefficient \ref{subsec:pearson_cc} and normalized cross correlation \ref{subsec:normalized_cross_correlation} for evaluation on the test dataset \ref{subsubsec:test_dataset}. For the training loss we have selected the mean square error (MSE) function (\citet{alpaydin2020introduction}). We during training and backpropagation steps we then aim to minimize the function:

$$L(\hat{R}, R) = \sum_{i=1}^{N}\sum_{t=1}^{T}\left(\hat{R}_i(t) - R_i(t)\right)^2$$

Where $\hat{R}, R \in \R^{N \times M_{out} \times T}$ are the matrices of predicted respectively target neuronal responses where $N$ is number of training experiments, $M_{out}$ is the total number of output neurons and $T$ is time duration of the experiment.

It is worth noting that same as in the case of the evaluation in visual system identification task there is no strict consensus on the selection of the proper loss function. We have selected MSE as one of the loss functions. The reasons for selection of such a loss function are that it is one of the most used, well-interpreted and easy to implement loss functions in the machine learning and even in the signal identification tasks as more profoundly studied for example in work from \citet{wang2009mse, soderstrom2018errors}. Such loss function was for example also partially used in the similar problem study by \citet{antolik2016local}. On the other hand it is also work noting that in for a wide range of the similar studies there is for example Poisson loss function (\citet{terven2024lossfunctionsmetricsdeep}) applied instead for example in studies by \citet{Wang2023towards, sinz2018stimulus}. The Poisson loss might be a better option as it is designed for the count discrete data which in the spiking counts are. As stated before we have empirically selected the MSE as it is one of the safe error function choices and at the beginning of our study it has been the metric that generally performed well without strenuous fine tuning and tackling with the model parameters. We are aware of this possible limitations and it would be appropriate to test the model training using other loss function variants such as Poisson loss in the future study. However, due the time constrains that we have to finish our thesis we decided to omit this ideal loss function selection in this study as we do not expect it would have massive impact on the study results.

As an optimizer function we have selected Adam optimization method (\citet{kingma2017adammethodstochasticoptimization}) as one of the state-of-the-art optimizations in the DNN model application. 

\subsection{Base Model Architecture}
\label{subsec:base_model_architecture}
Now, we will focus on the description of the base architecture of our model. This architecture serves as the basis of our model and throughout the research we subsequently implemented additional modules to improve the model performance and its biological plausibility.

Since now we will call this model as \emph{simple model} and we will build additional enhancements on this variant.

The simple model is an RNN that should reflect the biological architecture of the selected layers IV and II/III of the V1. Basically the architecture is the same as the one from our template SNN model from \citet{antolik2024comprehensive} that also serve as our artificial dataset source. However, there has been some simplifications of the model as application of such a high amount of the biological plausibility is very demanding and would require much more research out of the bounds of this thesis. The idea of the whole project is to integrate the original SNN from \citet{antolik2024comprehensive} with our model to improve the whole simulation.

The input of our model is a sequence of LGN neuronal spiking responses. These sequences we note as $s_{ON}$ and $s_{OFF}$ distinguishing neuronal populations of ON and OFF LGN cells (\ref{fig:on_off_cells}, \ref{sec:lgn}). These are passed to the 2 RNN layers representing layer IV of V1. We separate the layer IV to inhibitory (noted as $I_4$) and excitatory (noted as $E_4$) sublayer. These layers consists of artificial neurons where each should correspond to exactly one neuron from the original SNN of the appropriate layer. As mentioned in Section \ref{subsubsec:subset_selection} we select only subset of these neurons from original SNN as it is computationally unsuitable to create the 1:1 model architecture of the original SNN. These sublayers are interconnected with all-to-all connections within each other. There are also an all-to-all connections within each sublayer. The output of $E_4$ is passed in all-to-all manner to other 2 sublayers representing both excitatory and inhibitory populations of the layer II/III. These layers we will denote as $E_{2/3}$ and $I_{2/3}$. There is the same interconnectivity and self-connectivity in all-to-all manner as in the layer IV. In addition there is a recurrent connection from $E_{2/3}$ to both sublayers of layer IV ($E_4$ and $I_4$) also in the all-to-all manner. This recurrent connection is a simplification of the biological architecture introduced in the model \citet{antolik2024comprehensive}. The idea is in omitting the feedback connections from the layers V and VI and replacing them with direct layer II/III connection. The overall structure is inspired by the real V1 cortical architecture that is briefly described in the Section \ref{sec:v1} with the simplifications applied in the SNN model by \citet{antolik2024comprehensive}. In addition there are simplifications applied in our model. These simplifications are as mentioned before the selection of only a random subset of the neurons from the model by \citet{antolik2024comprehensive} that are modeled. Alongside with this there are no spatial constraints applied on the connections. This means that the neurons are connected all-in-all manner. This is not valid neither biologically nor in the SNN model where the connections are defined stochastically with the distribution where the probability of the connection decreases with the spatial distance and orientation selectivity. This limitation are planned to be replaced in the future studies building on our ground model as they will allow us apart from the improving the biological plausibility also scale up our model as the number of connections are directly linked with the number of parameters that the model needs.

The output of the model are all the outputs of the layers as these should represent the appropriate neuronal responses in the given time step of each modeled neuronal population. The output in time $t$ is then thus in the format:

$$\hat{R}(t) = (E_4(t); I_4(t); E_{2/3}(t); I_{2/3}(t)) $$

In this ground RNN model we use classical RNN neurons with the activation function tanh. We have selected the tanh function as the layer output should correspond to the neuronal responses that should be non-negative as they should represent number of spikes that the neuron elicited in the given time bin.

Alongside with this we differentiate between the inhibitory and excitatory neurons \ref{subsec:synaptic_transmission}. The artificial neurons of RNN in excitatory layers $E_4$ and $E_{2/3}$ are weights constrained to be non-negative throughout all training that should represent the excitatory (empowering) signal from these neurons. On the other hand, the inhibitory neurons in layers $I_4$ and $I_{2/3}$ have weights strictly constrained to be non-positive, representing the inhibitory effect of these cells. This should lead to even higher biological plausibility. These constraints are forced during each optimizer step. In case the threshold is passed during the optimizer step the weights are reset to the threshold value ensuring this constraint is fulfilled during the whole training procedure.

The training of the model was done the following way. At the beginning we pass the appropriate input values to the network starting with the processing in layer IV. As this layer apart from the LGN neuronal responses works with the recurrent input from the other cortical layers we pass these values from our artificial dataset from the previous time steps. This we apply for all the recurrent connections in the model. We perform one forward step, compute the loss between the neuronal predictions in each layer, perform backpropagation step and optimizer step. After the model weight update we do not pass the model outputs as the recurrent inputs for the next time step, but we reset these values with the corresponding target value from the artificial dataset. This ensures the model gets the correct input values in each time step and also speeds up the training. As we do not really have any hidden parameters in the simple model and we know exactly how should each neuron in our model respond on all the stimuli, we can exploit this non-typical property of the RNN training for facilitation of the model training. Since we are working with the RNN model it also makes sense to try to apply the hidden time steps. On the other words, to perform multiple forward steps and take the last time step as the model predictions in the target time step. This variant makes it possible to use the backpropagation through time and have some kind of memory encoded in the model. We have implemented this option, but while testing it did not show any promising results and we won't apply this option in the future study. It is worth noting that in the future this functionality might be useful and might improve the model performance.

The evaluation of the model is simple though. We initiate the recurrent inputs with the first time step targets from our artificial dataset and then perform the forward steps for all the time steps when taking only the LGN inputs and not reseting the recurrent inputs anymore. This means that during the evaluation we can test whether our model is able to predict the whole series of the time steps in the experiment, eventhough, it was never really presented to the model during the training.

\subsection{Additional Modules}
\label{subsec:additional_modules}
Apart from the simple model described in the Section \ref{subsec:base_model_architecture} we added multiple additional modules that enhance the model performance and architecture using additional biological constraints. We will describe the modules in the order in which we was adding the modules to improve the model performance. So, the reader can see the process of the model development and the ideas behind each of the module addition. In theory it does not refrain the user from omitting the previously defined modules but it typically does not make sense as they were build to fine-tune the previous version of the model.

\subsubsection{Feed-forward Neuron Module}
\label{subsubsec:dnn_neuron}
The first and arguably also the most impactful complement is the replacement of the current tanh neuron activation functions of the simple model using the shared small trainable feed-forward DNN module that should better reflect the complex behavior of the neuron. We use 4 separate DNN modules for each output neuronal population layer ($E_4$, $I_4$, $E_{2/3}$, $I_{2/3}$). This should also differentiate different neuronal types in the system. As in the source model of our artificial dataset the other types are omitted thus the model is simplified by this.
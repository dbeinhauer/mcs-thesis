\chapter{Methods}
\label{chap:methods}

In this chapter, we begin by describing the artificial stimulus dataset used to train our model, including a brief overview of the model from which the dataset originates. We then detail the preprocessing steps applied to the dataset to make it compatible with our model. The final and major section of this chapter focuses on our biologically plausible recurrent neural network (RNN) model of the primary visual cortex (V1).

\section{Spiking Model of Cat Primary Visual Cortex}
\label{sec:cats_model}
A major challenge in computational neuroscience, particularly in system identification tasks \ref{sec:system_identification}, is the limited availability and quality of biological data. This issue becomes even more pronounced when developing models using deep neural networks (DNNs), as discussed in Section \ref{subsec:deep_learning_approach}. The noise and expense of biological data acquisition motivate the use of artificial datasets. In our work, we utilize data generated by a state-of-the-art spiking model of the cat V1 developed by \citet{antolik2024comprehensive}.

Originally, we intended to fine-tune the model using real data from multielectrode array recordings in macaque V1, spanning tens of thousands of images. However, due to time constraints, this was not feasible within the scope of this thesis. Integration of real data remains a promising direction for future research.

\subsection{Spiking Model Description}
\label{subsec:spiking_model_description}

The artificial dataset is derived from a biologically detailed spiking model of cat V1 introduced by \citet{antolik2024comprehensive}. Designed to simulate visual experiments, this model aims to address the challenges inherent in biological data acquisition.

The model is implemented as a spiking neural network (SNN) using the exponential integrate-and-fire neuron model (\citet{FourcaudTrocm11628}), an extension of the leaky integrate-and-fire model (see Section \ref{subsec:spiking_neural_nets}). It captures the spatial layout of cortical layers IV and II/III in a 5.0 x 5.0 mm patch corresponding to the \emph{area centralis} of the retina, responsible for high-acuity vision (see Section \ref{subsec:retina}).

The model incorporates various biological properties, such as differentiation between excitatory and inhibitory neurons, synaptic depression, and local connectivity among neurons with similar orientation preferences (Section \ref{subsec:receptive_field}). It has been validated against a wide range of experimental data, including responses to white-noise and natural stimuli (Section \ref{subsec:stimulus}). Compared to other models, it is distinguished by its comprehensive inclusion of biological features (\citet{antolik2024comprehensive}).

A particularly notable strength of the model is its ability to reproduce spontaneous neuronal activity without artificial noise sources. This enables it to replicate trial-to-trial variability in stimulus responses (Section \ref{sec:evaluation_methods}), as demonstrated in \citet{baudot_animation_2013}. These capabilities were validated using blank gray stimuli (\citet{PAPAIOANNOU1972558}), making the model a valuable source of biologically plausible data.

Despite its strengths, the model remains a simplification of the biological system. For instance, feedback from cortical layers V and VI and the diversity of neuron types within layers are not explicitly modeled (Section \ref{sec:v1}). Furthermore, the recurrent pathway from layer II/III to IV serves as a proxy for these excluded modulatory circuits.

\subsubsection{Model Architecture}
\label{subsubsec:spiking_cat_architecture}
The model is substantially downsampled relative to the biological system. It comprises 108,150 neurons and approximately 155 million synapses, representing roughly 10\% of the neuronal density in cat V1 (\citet{beaulie1989number}). Neurons are distributed across layers according to biological data, with excitatory and inhibitory neurons present in a 4:1 ratio (\citet{bealuliee1992quantitative, markram_interneurons_2004}). The model includes both feedforward and recurrent connections within and between layers.

It also incorporates a simplified lateral geniculate nucleus (LGN) model (Section \ref{sec:lgn}), with distinct ON and OFF pathways (Figures \ref{fig:early_vis_processing} and \ref{fig:on_off_cells}). These 7,200 LGN neurons are spatially aligned with the receptive fields of their V1 counterparts. For further details, see \citet{antolik2024comprehensive}.

\section{Artificial Dataset}
\label{sec:artificial_dataset}
Our dataset consists of spiking activity generated by the model described above \ref{sec:cats_model}. It includes responses from 14,400 LGN neurons and 108,150 V1 neurons, recorded at 1 ms resolution. The data is organized into sequences corresponding to multiple experiments, each comprising alternating blank and natural image stimuli. Due to the model's comprehensiveness, unnecessary information was removed during preprocessing, as described below.

\subsection{Stimulus Sequence Structure}
\label{subsec:stimulus_sequence}
The dataset is divided into \emph{stimulus sequences}, each comprising alternating intervals of a blank gray screen and a natural stimulus image neuronal responses. Each pair forms a \emph{stimulus pair}, where the blank interval precedes the stimulus to ensure that only spontaneous activity is present before the image is shown.

Each stimulus sequence begins with a blank interval of duration $t_{\text{blank}}$, followed by a stimulus interval of duration $t_{\text{stim}}$. These pairs are repeated periodically. Blank intervals serve to reset neural activity, preventing carryover effects from previous stimuli. Notably, the final blank interval following the last stimulus is omitted, introducing a minor inconsistency.

\subsection{Experiment Definition}
\label{subsec:experiment}
To facilitate processing and reduce memory demands, stimulus sequences are divided into smaller units called \emph{experiments}. Each experiment consists of the second half of a blank interval, the corresponding stimulus interval, and the first half of the following blank interval. This ensures that the neural activity at the boundaries of the stimulus reflects spontaneous states.

\begin{defn}[Experiment]
    Let $S$ be a stimulus sequence consisting of $N \in \mathbb{N}$ pairs of blank and stimulus intervals $(b_i, s_i)$. Let $l_b$ and $l_s$ be the lengths of the blank and stimulus intervals, respectively. Then, for $j \in [1, 2, \dots, N-1]$, an experiment is defined as:

    $$
    \left(b_j\left[\frac{l_b}{2}:l_b\right]; s_j; b_{j+1}\left[0:\frac{l_b}{2}\right]\right)
    $$

    The final experiment omits the trailing blank interval:

    $$
    \left(b_N\left[\frac{l_b}{2}:l_b\right]; s_N\right)
    $$
\end{defn}
\label{def:experiment}

To maintain consistency, the first half of the initial blank interval is omitted. While this results in minimal data loss, it facilitates uniform formatting. Experiments lacking a post-stimulus blank interval are padded with zeros.

\subsection{Dataset Preprocessing}
\label{subsec:dataset_preprocess}
The original dataset includes detailed simulation data, much of which is irrelevant for training. We extract only the spike trains (Section \ref{subsec:spike_trains}) and segment them into experiments, reducing memory and computation demands significantly.

To streamline this process, we have developed a dedicated data processing pipeline that converts raw simulation output into the final experiment-based format used by our model. This pipeline is modular and reusable, allowing efficient processing of future datasets generated by the same spiking model. As such, it facilitates further experiments and model iterations without redundant implementation effort.

\subsection{Dataset Description}
\label{subsec:dataset_description}

After the raw dataset generated from the model by \citet{antolik2024comprehensive} undergoes the preprocessing steps described in Section \ref{subsec:dataset_preprocess}, we obtain the final dataset used in our model. The basic unit of this dataset is the experiment (Section \ref{subsec:experiment}). Each experiment is represented by six matrices corresponding to different neuronal populations. Two matrices represent the spike trains of ON and OFF LGN neurons (denoted as $s_{ON}$ and $s_{OFF}$), while the remaining four correspond to layer IV and layer II/III neurons, further divided into excitatory and inhibitory types: $r_{E_{4}}$, $r_{I_{4}}$, $r_{E_{2/3}}$, and $r_{I_{2/3}}$. The number of neurons in each population is defined in Section \ref{subsubsec:spiking_cat_architecture}, matching the configuration in \citet{antolik2024comprehensive}.

Each matrix has the format $\mathbb{R}^{N \times M \times T}$, where $N$ is the number of experiments, $M$ is the number of neurons in the given population, and $T$ is the number of time steps in each experiment (see Section \ref{subsec:experiment} for experiment definition).

In our artificial dataset, the blank stage lasts for 150 ms, and the natural stimulus stage lasts for 560 ms, both recorded in 1 ms bins. Thus, the spike train for a single neuron in an experiment spans 710 ms, composed of 75 ms of a blank interval before the stimulus, 560 ms of stimulus presentation, and 75 ms of the subsequent blank interval. The spike train is encoded in binary format: a 1 indicates a spike at a given time step, and a 0 indicates no spike. For some experiments at the end of a stimulus sequence, the final blank interval is missing. To ensure uniform experiment duration, we pad these with zeros (no spikes).

Although this padding does not accurately reflect biological spontaneous activity (which is non-zero, see Section \ref{subsec:spiking_model_description}), it affects only a minority of cases. We chose this pragmatic solution for its simplicity and minimal impact on model performance, avoiding additional complexity and computational overhead.

In classical machine learning, datasets are typically split into training, validation, and test subsets, often with randomized sampling. However, our use case is complicated by trial-to-trial variability in neuronal responses, necessitating multiple repeated trials in the validation and test datasets (see Section \ref{sec:evaluation_methods}). Because this variability arises mainly from spontaneous neural activity (\citet{antolik2024comprehensive}), we chose not to include multiple trials in the training set. Instead, we prioritized generating a wider variety of single-trial responses to diverse natural stimuli. This design choice balances several practical factors: dataset size, memory efficiency, training speed, and simulation time.

As a result, we do not apply randomized splits across the dataset but instead rely on a structured division between training and evaluation datasets.

\subsubsection{Train Dataset}
\label{subsubsec:train_dataset}

The training dataset consists of 500 stimulus sequences, each containing 100 pairs of blank and natural stimuli, all presented in a single trial. After preprocessing and experiment segmentation (Section \ref{subsec:experiment}), this yields a total of 50,000 unique experiments for training.

\subsubsection{Test and Validation Dataset}
\label{subsubsec:test_dataset}

The validation and test datasets comprise 18 stimulus sequences, each with 50 stimulus pairs repeated over 20 trials. After preprocessing, this results in 900 experiments, each with 20 trial repetitions. We randomly divide this set into validation and test subsets using a 1:9 ratio.

\subsubsection{Neuron Subset Selection}
\label{subsubsec:subset_selection}

To build a biologically plausible RNN model that mirrors the structure in \citet{antolik2024comprehensive}, one would ideally create a one-to-one mapping between model neurons and biological neurons. However, given the lack of structural constraints (e.g., known synaptic weights) and the impracticality of modeling over 100,000 neurons, we adopt an all-to-all connectivity scheme with neuron subsampling.

We randomly select 10\% of neurons from each population to reduce model complexity and memory consumption, making the model more tractable during training and fine-tuning. To mitigate sampling bias, we run experiments using 20 different random neuron subsets. Empirical testing has shown that using this 10\% subset produces performance metrics similar to those obtained with larger neuron subsets. Further evaluation of this design choice is presented in the next chapter.

\section{Model Description}
\label{sec:model_description}

In this section we will focus on the definition of the ground model and then on the biologically inspired extensions that we consequently applied to improve the model performance.

The task of the model is the visual system identification as described in Section \ref{sec:system_identification}. More profoundly our aim is design the RNN model that based on the input sequence that consists of the LGN neurons (\ref{sec:lgn}) responses from our artificial dataset described in the previous Section \ref{sec:artificial_dataset} predicts the appropriate responses of the selected V1 neurons whose responses we have in our dataset as mentioned in Section \ref{sec:artificial_dataset}. In terms of the visual system identification we have in each time step $t$ of the input experiment sequence \ref{subsec:experiment} a vector of neuronal responses from the LGN ON and OFF cells that serves as an input stimuli. Let's denote $s(t)$ as our vector of input stimuli (using notation of appropriate neuronal responses used in the Section \ref{sec:artificial_dataset}) in time step $t$: 

\begin{equation*}
    s(t)=\left(s_{ON}(t); s_{OFF}(t)\right)
\end{equation*}

Our task is to find the model $f$ that is a biologically plausible RNN that predicts the neuronal responses from the V1 neuronal responses on the stimulus in time $t$ from $s(t)$. Let's denote this as:

\begin{equation*}
    \hat{r}(t) = f(s(t))
\end{equation*}

Where $\hat{r}(t)$ consists of:

\begin{equation*}
    \hat{r}(t) = \left(\hat{r}_{E_4}(t); \hat{r}_{I_4}(t); \hat{r}_{E_{2/3}}(t); \hat{r}_{I_{2/3}}(t)\right)
\end{equation*}

Also note that we will mark the predictions using the hat $\hat{.}$ symbol and without this symbol we will mark the targets from our artificial dataset.

The aim of the task is to to minimize the difference between predictions $\hat{r}(t)$ and targets $r(t)$ and to accurately capture the spatiotemporal dynamics of the single neuronal responses. As mentioned in Section \ref{sec:evaluation_methods}, there is currently no consensus in the ideal evaluation metrics, thus we will use Pearson's correlation coefficient \ref{subsec:pearson_cc} and normalized cross correlation \ref{subsec:normalized_cross_correlation} for evaluation on the test dataset \ref{subsubsec:test_dataset}. For the training loss we have selected the mean square error (MSE) function (\citet{alpaydin2020introduction}). We during training and backpropagation steps we then aim to minimize the function:

$$L(\hat{R}, R) = \sum_{i=1}^{N}\sum_{t=1}^{T}\left(\hat{R}_i(t) - R_i(t)\right)^2$$

Where $\hat{R}, R \in \R^{N \times M_{out} \times T}$ are the matrices of predicted respectively target neuronal responses where $N$ is number of training experiments, $M_{out}$ is the total number of output neurons and $T$ is time duration of the experiment.

It is worth noting that same as in the case of the evaluation in visual system identification task there is no strict consensus on the selection of the proper loss function. We have selected MSE as one of the loss functions. The reasons for selection of such a loss function are that it is one of the most used, well-interpreted and easy to implement loss functions in the machine learning and even in the signal identification tasks as more profoundly studied for example in work from \citet{wang2009mse, soderstrom2018errors}. Such loss function was for example also partially used in the similar problem study by \citet{antolik2016local}. On the other hand it is also work noting that in for a wide range of the similar studies there is for example Poisson loss function (\citet{terven2024lossfunctionsmetricsdeep}) applied instead for example in studies by \citet{Wang2023towards, sinz2018stimulus}. The Poisson loss might be a better option as it is designed for the count discrete data which in the spiking counts are. As stated before we have empirically selected the MSE as it is one of the safe error function choices and at the beginning of our study it has been the metric that generally performed well without strenuous fine tuning and tackling with the model parameters. We are aware of this possible limitations and it would be appropriate to test the model training using other loss function variants such as Poisson loss in the future study. However, due the time constrains that we have to finish our thesis we decided to omit this ideal loss function selection in this study as we do not expect it would have massive impact on the study results.

As an optimizer function we have selected Adam optimization method (\citet{kingma2017adammethodstochasticoptimization}) as one of the state-of-the-art optimizations in the DNN model application. 

\subsection{Base Model Architecture}
\label{subsec:base_model_architecture}
Now, we will focus on the description of the base architecture of our model. This architecture serves as the basis of our model and throughout the research we subsequently implemented additional modules to improve the model performance and its biological plausibility.

Since now we will call this model as \emph{simple model} and we will build additional enhancements on this variant.

The simple model is an RNN that should reflect the biological architecture of the selected layers IV and II/III of the V1. Basically the architecture is the same as the one from our template SNN model from \citet{antolik2024comprehensive} that also serve as our artificial dataset source. However, there has been some simplifications of the model as application of such a high amount of the biological plausibility is very demanding and would require much more research out of the bounds of this thesis. The idea of the whole project is to integrate the original SNN from \citet{antolik2024comprehensive} with our model to improve the whole simulation.

The input of our model is a sequence of LGN neuronal spiking responses. These sequences we note as $s_{ON}$ and $s_{OFF}$ distinguishing neuronal populations of ON and OFF LGN cells (\ref{fig:on_off_cells}, \ref{sec:lgn}). These are passed to the 2 RNN layers representing layer IV of V1. We separate the layer IV to inhibitory (noted as $I_4$) and excitatory (noted as $E_4$) sublayer. These layers consists of artificial neurons where each should correspond to exactly one neuron from the original SNN of the appropriate layer. As mentioned in Section \ref{subsubsec:subset_selection} we select only subset of these neurons from original SNN as it is computationally unsuitable to create the 1:1 model architecture of the original SNN. These sublayers are interconnected with all-to-all connections within each other. There are also an all-to-all connections within each sublayer. The output of $E_4$ is passed in all-to-all manner to other 2 sublayers representing both excitatory and inhibitory populations of the layer II/III. These layers we will denote as $E_{2/3}$ and $I_{2/3}$. There is the same interconnectivity and self-connectivity in all-to-all manner as in the layer IV. In addition there is a recurrent connection from $E_{2/3}$ to both sublayers of layer IV ($E_4$ and $I_4$) also in the all-to-all manner. This recurrent connection is a simplification of the biological architecture introduced in the model \citet{antolik2024comprehensive}. The idea is in omitting the feedback connections from the layers V and VI and replacing them with direct layer II/III connection. 

Formally the computation done by an artificial neurons in one arbitrary layer of the base model described above is defined as:

\begin{defn}[Base Neuronal layer]
    Let's mark $L$ as the neuronal layer that contains $m_{h} \in \N$ neurons. Let $x \in \R^{m_{in}}$ be a vector of all corresponding neuronal responses that are passed as to a neuronal layer $L$ where $m_{in} \in \N$ is a total number of the input neurons. And let $W_{ih} \in \R^{m_{h} \times m_{in}}$ and $b_{ih} \in \R^{m_{h}}$ be input weights matrix and bias vector of the layer $L$. Let $h \in \R^{m_h}$ be the vector of neuronal responses of each neuron from layer $L$ in the previous time step and its corresponding weights matrix $W_{hh} \in \R^{m_{h} \times m_{h}}$ and bias $b_{hh} \in \R^{m_h}$. Let's mark $f: \R^{m_h} \to \R^{m_h}$ to be an arbitrary activation function. Then vector of neuronal responses $h'$ of the layer $L$ on the input neuronal responses $x$ and $h$ is defined as:

    $$h' = f\left(W_{ih}x + b_{ih} + W_{hh}h + b_{hh}\right)$$
\end{defn}
\label{def:base_neuron}

The activation function $f$ can be basically chosen as any function that fulfils the condition from the definition. We will focus on the activation function more profoundly in the subsequent text. Please note that in the following text there will be multiple times used the notation for the activation function in vector format as in the definition \ref{def:base_neuron}. The reason for this notation is mainly to clearly define the input and output size but if not stated otherwise these functions are typically the activation functions $\R \to \R$ and vector notation then just means that we apply this function on each element of the vector separately (the elements in the layer typically does not influence each other).

The overall structure is inspired by the real V1 cortical architecture that is briefly described in the Section \ref{sec:v1} with the simplifications applied in the SNN model by \citet{antolik2024comprehensive}. In addition there are simplifications applied in our model. These simplifications are as mentioned before the selection of only a random subset of the neurons from the model by \citet{antolik2024comprehensive} that are modeled. Alongside with this there are no spatial constraints applied on the connections. This means that the neurons are connected all-in-all manner. This is not valid neither biologically nor in the SNN model where the connections are defined stochastically with the distribution where the probability of the connection decreases with the spatial distance and orientation selectivity. This limitation are planned to be replaced in the future studies building on our ground model as they will allow us apart from the improving the biological plausibility also scale up our model as the number of connections are directly linked with the number of parameters that the model needs.

The output of the model are all the outputs of the layers as these should represent the appropriate neuronal responses in the given time step of each modeled neuronal population. The output in time $t$ is then thus in the format:

$$\hat{R}(t) = (E_4(t); I_4(t); E_{2/3}(t); I_{2/3}(t)) $$

We should mention that we treat our task as a regression task eventhough it might make sense to treat is as classification task. This is due the non-clarity of the ideal loss and evaluation metrics selection. It is quite common in this field to perform the regression task like we do in our study.

\subsubsection{Time Bins Merging}
\label{subsubsec:time_bins_merging}

Additionally, as the training of the NNs is time consuming and dealing with the sequences in resolution 1ms is both time and memory demanding we have after some brief dataset observation and empirical decisions based on the capturing temporal behavior alongside with the omitting the noisiness of our data since we are working only with single trial data (this problem with noisiness was already discussed in Section \ref{sec:evaluation_methods}) we decided to merge the time bins from 1ms resolution to 20ms resolution. As this resolution reasonably captures the temporal behavior of the neuronal responses, is much less time and memory consuming and also we omit the noisiness of the data. Further will this decision be discussed in the following chapter where we will discuss the results of our study.

\subsubsection{LeakyTanh Activation Function}
\label{subsubsec:leakytanh}

In the definition of the ground model layer computation there has been mentioned that basically there can be selected any activation function that fulfills the conditions \ref{def:base_neuron}.

In this ground RNN model we use classical RNN neurons with the custom activation function that we call \emph{LeakyTanh}. This function was designed by other student Richard Kraus who is currently working on the extensition of the model as his bachelor's thesis that should apply the spatial constrains to our model. The idea behind the selection of activation function and motivation to develop custom one is that since our model predictions are spiking rates of the individual neurons its value should never be negative as it does not make biologically sense. Additionally as we are working with the 20ms time resolution and as mentioned in \citet{dayan2005theoretical} there is at most 1 spike in 1ms then theoretically there should be maximally 20 spikes in one time bin. Thus, it would make sense to force the activation function to predict values in interval from 0 to 20. While doing some additional research on the dataset we have also found out that maximal number of spikes throughout our dataset in one time bin is only 4 and in majority cases these responses are still binary (this was also one of the reasons why we have chosen this resolution) the final activation function should predict the values in ideally much narrower interval. There has been several activation functions that we have tried such as ReLU and its variants (also constraints), tanh and several other custom variants. Finally, we have empirically selected the LeakyTanh function as the training was the most stable and behaved in a predictable way in comparison to others. Our hypothesis is that our custom function behaves the best because, it is fully differentiable in $\R$, it is constrained only from the bottom in contrast to other activation functions that we tried that are constrained from both sides. These properties has been proven to not work well while training the DNN models (\citet{shiv2022activation, nwankpa2018activationfunctionscomparisontrends}). On the other hand non-constrained functions also did not perform well (eventhough it would be ideal) in comparison to constrained. There was a problem mainly with the gradients reaching infinity during training. Although, we have solved these problems using gradient clipping it still did not prove to be better option. It is also worth noting that we are still using gradient clipping to prevent the model of the exploding gradients as our activation function is not constrained from the top. This clipping has however high threshold and we expect we do not need to apply it also never as the LeakyTanh function is designed the way that is it almost never possible. Formally the LeakyTanh is defined as:

\begin{defn}[LeakyTanh]
    Let us define the interval of our main interest $[0, m]$. Let us define scale offset $c = \frac{m}{2}$, parameter that ensures the function is positive. Next define steepness parameter $s \in \R$ specifying steepness of the core $\tanh$ function. Finally, define leakage function $f(x) = \text{softplus}(x, \beta)$ with parameter $\beta \in \R$ and leakage strength $l \in \R$ that serves as a leakage factor of our tanh function to ensure non-boundness. The LeakyTanh function is then defined as:
        $$\text{LeakyTanh}(x; c, s, l, \beta) = c + c \cdot \tanh(s \cdot x) + l \cdot \text{softplus}(x, \beta)$$
\end{defn}
\label{def:leakytanh}

Note that $\text{softplus}(x, \beta)$ function with parameter $\beta \in \R$ is a function serving as a continuous replacement for the ReLU function (mentioned for example in the paper from \citet{nwankpa2018activationfunctionscomparisontrends}). In our model we use LeakyTanh with parameters $c=2.5$ to prefer mainly the values in interval $[0, 5]$ as explained above. Additionally, we have chosen the parameters $l=0.001$, $s=0.2$ and $\beta=20$. It is worth noting that these parameters has been profoundly studies by Richard Kraus and our contribution to this hyperparameters selection and activation function development was mainly in terms of stating the expected properties by the dataset examination conducted by us and by testing the function parameters on our developed model and comparison to results and model behavior while using other activation function. As stated before, we have decided to use this activation function as it empirically seemed as the most reasonable one out of all tested. 

The comprehensive study on the ideal activation function can be for example found in the papers from \citet{shiv2022activation, nwankpa2018activationfunctionscomparisontrends}.

\subsubsection{Excitatory/Inhibitory Neurons Differentiation}
\label{subsubsec:exc_inh_differentiation}

Alongside with this we differentiate between the inhibitory and excitatory neurons \ref{subsec:synaptic_transmission}. The artificial neurons of RNN in excitatory layers $E_4$ and $E_{2/3}$ are weights constrained to be non-negative throughout all training that should represent the excitatory (empowering) signal from these neurons. On the other hand, the inhibitory neurons in layers $I_4$ and $I_{2/3}$ have weights strictly constrained to be non-positive, representing the inhibitory effect of these cells. This should lead to even higher biological plausibility. These constraints are forced during each optimizer step. In case the threshold is passed during the optimizer step the weights are reset to the threshold value ensuring this constraint is fulfilled during the whole training procedure.

Formally the constrained function applied after each weights update on the weights corresponding to excitatory layers $E_4$ and $E_{2/3}$:

\begin{defn}[Weight Constraints]
    Let $w_E, w_I \in \R$ be a weight of on arbitrary neuron from the arbitrary excitatory layer $E_4$ or $E_{2/3}$ respectively arbitrary inhibitory layer $I_4$ or $I_{2/3}$  of our model. Then after each update of these weights there are applied the following functions. For excitatory neurons (when $w_{E_{\text{new}}}$ labels the updated weight):
    \begin{equation*}
        w_{E_{\text{new}}} =
        \begin{cases}
            w_{E} & \text{if $w_E \geq 0$} \\
            0 & \text{else} \\
        \end{cases}
    \end{equation*}
    
    And for inhibitory neurons (where $w_{I_{\text{new}}}$ labels the updated weight):
    \begin{equation*}
        w_{I_{\text{new}}} =   
        \begin{cases}
            w_{I} & \text{if $w_I \leq 0$} \\
            0 & \text{else} \\
        \end{cases}
    \end{equation*}
\end{defn}


\subsubsection{Training of the Model}
\label{subsubsec:training_model}

The training of the model was done the following way. At the beginning we pass the appropriate input values to the network starting with the processing in layer IV. As this layer apart from the LGN neuronal responses works with the recurrent input from the other cortical layers we pass these values from our artificial dataset from the previous time steps. This we apply for all the recurrent connections in the model. We perform one forward step, compute the loss between the neuronal predictions in each layer, perform backpropagation step and optimizer step and at the end we apply the weights constraints to fulfill the excitatory/inhibitory differentiation \ref{subsubsec:exc_inh_differentiation}. After the model weight update we do not pass the model outputs as the recurrent inputs for the next time step, but we reset these values with the corresponding target value from the artificial dataset. This ensures the model gets the correct input values in each time step and also speeds up the training. As we do not really have any hidden parameters in the simple model and we know exactly how should each neuron in our model respond on all the stimuli, we can exploit this non-typical property of the RNN training for facilitation of the model training. Since we are working with the RNN model it also makes sense to try to apply the hidden time steps. On the other words, to perform multiple forward steps and take the last time step as the model predictions in the target time step. This variant makes it possible to use the backpropagation through time (\citet{webos1990btt}) and have some kind of memory encoded in the model. We have implemented this option, but while testing it did not show any promising results and we won't apply this option in the future study. It is worth noting that in the future this functionality might be useful and might improve the model performance.

\subsubsection{Evaluation of the Model}
\label{subsubsec:evaluation_model}

The evaluation of the model is simple though. We initiate the recurrent inputs with the first time step targets from our artificial dataset and then perform the forward steps for all the time steps when taking only the LGN inputs and not resetting the recurrent inputs anymore. This means that during the evaluation we can test whether our model is able to predict the whole series of the time steps in the experiment, eventhough, it was never really presented to the model during the training.

\subsection{Additional Modules}
\label{subsec:additional_modules}
Apart from the simple model described in the Section \ref{subsec:base_model_architecture} we added multiple additional modules that enhance the model performance and architecture using additional biological constraints. We will describe the modules in the order in which we was adding the modules to improve the model performance. So, the reader can see the process of the model development and the ideas behind each of the module addition. In theory it does not refrain the user from omitting the previously defined modules but it typically does not make sense as they were build to fine-tune the previous version of the model.

\subsubsection{Feed-forward Neuron Module}
\label{subsubsec:dnn_neuron}
The first and arguably also the most impactful complement is the replacement of LeakyTanh neuron activation functions of the simple model using the shared small trainable feed-forward DNN modules that should better reflect the complex behavior of the neuron. We use 4 separate DNN modules for each output neuronal population layer ($E_4$, $I_4$, $E_{2/3}$, $I_{2/3}$). This should also differentiate different neuronal types in the system. As in the source model of our artificial dataset the other types are omitted thus the model is simplified by this. At the end of each of these modules there is a final LeakyTanh activation function to maintain the properties of the predictions mentioned in the Section \ref{subsubsec:leakytanh}.

The feed-forward DNN neuron module is defined as:

\begin{defn}[DNN Joint Neuron Module]
    Let $n \in \N$ be a number of module layers and $s \in \N$ be a size of a single layer. Let there be a sequence of the following operations Layer Normalization (\citet{ba2016layernormalization}), fully-connected hidden NN layer and ReLU. This sequence represents one layer with exception to the first layer. The input size of the DNN joint neuron module is 1 as it is the output element of the vector of the base model defined in \ref{def:base_neuron}. Then the DNN neuron module $f: \R \to \R$ is defined as the sequence of operations:
    \begin{enumerate}
        \item Fully connected NN layer with input size $1$ and output size $s$.
        \item Sequence of $(n-1)$ repetitions of the layer block with operations:
            \begin{enumerate}
                \item Layer Normalization.
                \item Fully connected NN layer with input size $s$ and output size $s$.
                \item ReLU
            \end{enumerate}
        \item Fully connected NN layer with input size $s$ and output size $1$.
        \item LeakyTanh final activation function.
    \end{enumerate}

    Optionally, there might be a residual connection from that skips the whole module and is summed with the module result before the final activation function is applied.
\end{defn}
\label{def:dnn_joint}

Since now we will name model using this feed-forward neuronal module as \emph{DNN joint model}. The reason for the "joint" adverb is that this type of module does not differentiate excitatory and inhibitory outputs of the base RNN defined in the Section \ref{subsec:base_model_architecture} marked as simple model.

The idea behind this module is to capture the different behavior of different neuronal populations. Alongside with this there is also a motivation to learn more complex activation function than the simple ones that are typically used in the DNN model.

\subsubsection{Splitting Excitatory/Inhibitory Output in Base RNN}
\label{subsubsec:dnn_separate}
We will call another extension of our model as \emph{DNN Separate model}. This model is basically the same as the one as the DNN joint \ref{subsubsec:dnn_neuron} with one modification that is separation of the excitatory and inhibitory outputs during the base model RNN computation defined in \ref{def:base_neuron}. In the separate models we will separate the linear RNN cell computation into 2 part one part that performs the linear step for all excitatory input neurons and the other part that does the same for the inhibitory neurons. The result of the linear base RNN neuron is the vector with 2 elements (inhibitory and excitatory). These 2 elements are then passed together to the DNN neuron module basically the same as used in the DNN joint model defined in \ref{def:dnn_joint}. The only difference is that the module takes the input of size 2 (excitatory and inhibitory part) and in case of residual connection it first sums both input elements and then it sums this sum before the final activation function application. The difference between the definition \ref{def:dnn_joint} is then that the DNN separate module is a function $f: \R^2 \to \R$.

The idea behind this extension is to facilitate the model to distinguish between the excitatory and inhibitory neurons and separate their functionality from each other. The formal definition of the base model using "separate" variant is the following:

\begin{defn}[Separate Base RNN]
    Let's $L$ be an arbitrary neuronal layer containing $m_h \in \N$ neurons. Let $x_E \in R^{m_{in_E}}$ and $x_I \in R^{m_{in_I}}$ be input neuronal responses from the excitatory respectively inhibitory neurons of the layer $L$ and $m_{in_E}, m_{in_I} \in \N$ sizes of these populations. Also let $W_{ih_E} \in \R^{m_h \times m_{in_E}}$, $W_{ih_I} \in \R^{m_h \times m_{in_I}}$, $b_{ih_E} \in \R^{m_{in_E}}$ and $b_{ih_I} \in \R^{m_{in_I}}$ be input weights matrices and bias vectors of the excitatory respectively inhibitory appropriate input neurons. Let $h \in \R^{m_h}$ be the vector of neuronal responses for each neuron from layer $L$ from the previous time step and its corresponding weights $W_{hh} \in \R^{m_h \times m_h}$ and bias $b_{hh} \in \R^{m_h}$. Let's mark $f: \R^{2 \times m_h} \to \R^{m_h}$ to be an arbitrary activation function. Then vector of neuronal responses $h'$ of the layer $L$ on the input responses $x_E$, $x_I$ and $h$ is defined as:

    \begin{equation*}
        h' = 
        \begin{cases}
            f\left(W_{ih_E}x + b_{ih_E} + W_{hh}h + b_{hh}; W_{ih_I}x + b_{ih_I}\right) & \text{if $L$ is excitatory layer}\\
            f\left(W_{ih_E}x + b_{ih_E}; W_{ih_I}x + b_{ih_I} + W_{hh}h + b_{hh}\right) & \text{if $L$ is inhibitory layer}\\
        \end{cases}      
    \end{equation*}
\end{defn}
\label{def:separate_base_rnn}

As mentioned before as the activation function we use the same neuronal DNN module as in the DNN joint model with the slight modification that is that it takes input of size $2$ and sums the input excitatory and inhibitory values across in case of residual connection. We won't define this formally as the definition would be almost the same as in case of \ref{def:dnn_joint}.

\subsubsection{Recurrent Neuron Module}
\label{subsubsec:rnn_neuron_module}
Another extension we call \emph{RNN joint} or \emph{RNN separate} based on whether we use joint \ref{def:base_neuron} or separate base \ref{def:separate_base_rnn} model. This extension aims to bring some form of a "memory" of the neurons. The principle is the same as in the feed-forward DNN neuronal modules \ref{subsubsec:dnn_neuron} and \ref{subsubsec:dnn_separate} we just replace the feed-forward DNN with the RNN variant. As the small shared RNN module we use either LSTM (\citet{hochreiter1997lstm}) or GRU (\citet{cho2014gru}) RNN model variant. Formally, the RNN neurons are defined as:

\begin{defn}[RNN Neuron Module]
    Let $n \in \N$ be a number of layers of either LSTM or GRU units and $s \in \N$ be a size of a single layer. Let the input size of the module be $s_i \in \{1, 2\}$ to be $1$ in case of joint model variant and $2$ in case of the separate variant. Let $h \in \R^{m_h}$ be a vector of hidden states of the module where $m_h \in \N$ is given by the variant of the RNN section (either LSTM or GRU). The RNN neuron is then the given sequence of operations:
    \begin{enumerate}
        \item Apply fully connected NN with input size $s_i$ and output size $m_h$.
        \item Apply multi layer RNN variant based on our choice. Let the final hidden states mark as $h_{out} \in \R^{m_h}$.
        \item Apply final fully connected layer with input size $m_h$ and output size $1$.
        \item Apply final LeakyTanh activation function.
    \end{enumerate}

    Optionally, same as in the DNN neuron variant we might apply the residual connection that skips the module and adds the input before the final activation function call.
\end{defn}
\label{def:rnn_neuron_module}

The motivation behind this extension is the fact that the neurons adapt their behavior based on the short-term dynamics. Thus this might lead to better capturing temporal dynamics of the overall network spiking.

However, the introduction of the RNN neurons raises the concern about the training procedure and there needs to be some modifications done during the training as these models captures the temporal dynamics only when applying backpropagation through time (BPTT) (\citet{webos1990btt}). This problem has been already mentioned in the Section \ref{subsubsec:training_model} as our base model is also designed on the RNN architecture. In the mentioned case there, however, was not the necessity for BPTT as we in fact know all the hidden states in all time steps and we can reset the hidden states using the values from our dataset as we also have biological explanation of the base RNN hidden states. Although, this is not the case while using the neuronal modules. These are the new aspects in our system and we do not know what should be the ideal hidden values throughout the experiment sequence. We thus need to modify our training procedure to allow the model to perform the BPTT.

The modification to the training procedure described in \ref{subsubsec:training_model} is fairly simple. The only difference is that we will perform the backpropagation computation and the optimizer step only after the given number $T_{\text{back}} \in \N$ of forward steps representing the system neuronal responses in $T_{\text{back}}$ time steps and after this we perform the backpropagation computation, optimizer step and excitatory/inhibitory neurons constraint application. After this we do this repeatedly till the experiment sequence is not over. We perform the backpropagation computation only till the time step where the last optimizer call happened. We do not reset the hidden states of the RNN shared modules, however, we still reset the states of the base RNN model using the known neuronal responses as before. It is worth mentioning that in the original BPTT there should be all forward steps in the sequence performed and then there should be only one BPTT call as described before. However, as we want to capture mainly the short-term dynamics and as with the longering the time sequence for BPTT is significantly memory and time efficient that is our big concern as mentioned in the previous text. We cut the backpropagation step only for the given number of time steps. This variant is also called truncated backpropagation through time (TBPTT) (\citet{Williams1990tbptt}).

It is also worth noting that since we do not apply the optimizer time steps after each time step the training itself also takes more time and also as mentioned before especially memory consumption of this variant rises significantly with the number of time steps that we want to perform while using TBPTT. We thus need to find the balance between the efficiency and the temporal resolution. In this step there significantly rises the necessity to apply some kind of spatial constraint in neuronal connections to reduce number of model parameters that there currently is due to the all-to-all connections as mentioned in the Section \ref{subsec:base_model_architecture}. These constraints are also present in the original template SNN from \citet{antolik2024comprehensive} and are the case of the parallel research that should focus on the improvement of our model and is out of the scope of our thesis.

\subsubsection{Synaptic Depression}
\label{subsubsec:synaptic_depression}
The last extension of the model aims to include even more network short-term temporal adaptability to the model while using the principles of the synaptic depression (\citet{abbott1997syndepression}) that are also included in our template model by \citet{antolik2024comprehensive}. The idea of this principle is quite simple. The input neuronal connections adapt to the increased or decreased firing rate in the dynamical changes of the stimuli and thus modulate the postsynaptic signal that is transmitted from one neuron to other for example by amount of neurotransmitter efflux etc. (\ref{subsec:synaptic_transmission}).

In our model we apply this principle using the similar approach as in the shared modules of the neurons introduced in the previous extensions. We would have a small shared RNN module across the same ordered synaptic pairs. This will ensure that connection between each layers are independently modulated in time based on the stimuli. The highest influence of this phenomenon in biological network is in the synapses from LGN neurons to layer IV neurons. So, we first introduce the model that we call \emph{LGN only synaptic adaptation model} that applies synaptic depression only on the connections from LGN. The extension of this model is the one that we call \emph{synaptic adaptation model} that applies this principle to each synaptic pair population. In the model the synaptic depression module is applied as a preprocessing step of the input to the layer.

Formally the synaptic depression module is defined as:

\begin{defn}[Synaptic Depression Module]
    Let there be layer of neuronal population $L$ that takes input from neuronal populations from the set $\Lambda_{L}$. Let there be an arbitrary input layer $L_{in} \in \Lambda_{L}$ and let $x \in \R^{m_{in}}$ be a vector of neuronal responses of the layer $L_{in}$ in time $t$ where $m_{in} \in \N$ is a size of the layer. Let there $\sigma: \R^{m_{in}} \to \R^{m_{in}}$ be an synaptic depression module that fulfills the definition of an RNN joint module from the definition \ref{def:rnn_neuron_module}. Then input of the layer $L$ in time $t$ from the layer $L_{in}$ after application of the synaptic depression module is:
    $$x' = \sigma(x)$$
\end{defn}
\label{def:synaptic_depression}

As it was already mentioned in the Section concerning the RNN neuron modules \ref{def:rnn_neuron_module} there needs to be TBPTT applied as the synaptic depression modules are basically the same as the RNN neuron modules. Furthermore, the problem of the memory and computational cost is in case of this model even stronger since the number of parameters increased and thus with each additional time step in TBPTT the number memory usage increase rapidly. This is also reason why we use the smallest as possible GRU variants of the RNN modules as they are using less parameters and thus are less memory consuming. Furthermore, the LGN only synaptic depression module was developed due to this problems as it significantly reduces the number of synaptic depression modules and thus does not increase the memory usage that significantly. Overall this module seems to be problematic to use in a current state of our model as with the all-to-all connection it is not very feasible to train such a model in a reasonable time, scale and memory usage. However, we have high expectations that this module might have severe influence in the overall quality of the model temporal predictions and with the application of the spatial connectivity constraints this extension might significantly improve the model.
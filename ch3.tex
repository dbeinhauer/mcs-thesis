\chapter{Methods}
\label{chap:methods}
In this chapter we will first focus on the dataset of artificial stimuli that we use to train our model and briefly on the model from which the original dataset stem from. Followingly we will describe the preprocessing steps that has been done on this dataset to properly work on our model and finally in the major part of this chapter will be focused on our model of the primary visual cortex and its parts.

\section{Spiking Model of Cat Primary Visual Cortex}
\label{sec:cats_model}
The typical problem in computational neuroscience system identification task is the amount and quality of the data that we use to develop the model. This problem is especially important in case of developing model using DNNs as mentioned in the Section \ref{subsec:deep_learning_approach}. The noisiness and expensiveness of the biological data motivates us to use the artificial dataset generated from the state-of-the-art spiking model of the cat V1 from \citet{antolik2024comprehensive}. It is also worth noting that originally was the intention to fine-tune the model on the real data from the multielectrode array recordings from macaque V1 to tens of thousands of images. Unfortunately, due the time constraints for the completion of this thesis, we were not able to apply our model on the real data and this part of the work are planned for the future continuation of this project.

\subsection{Spiking Model Description}
\label{subsec:spiking_model_description}
The source of our artificial dataset is the state-of-the-art spiking model of the cat V1 developed by \citet{antolik2024comprehensive}. The model is developed with the aim to create the framework for the simulation of the visual experiments and thus partially solve the problems related to the biological data. The model is composed of SNN \ref{subsec:spiking_neural_nets} using the \emph{exponential integrate-and-fire} (\citet{FourcaudTrocm11628}) approach to model each neuron in the model which is an extension to a leaky integrate-and-fire model described in Section \ref{subsec:spiking_neural_nets}. The aim of the model is to capture as much of the biological properties as possible. The model architecture is composed to capture the spatial representation of the cortical areas in layer IV and layer II/III of the V1, within the 5.0 x 5.0 mm patch corresponding to the \emph{area centralis}, area of the retina responsible for high acuity vision (around the fovea) \ref{subsec:retina}. The model uses wide variety of the biological properties of the neurons and synapses, such as differentiation of the neuronal population to excitatory and inhibitory neurons, the synaptic depresion, architecture reflecting the cortical organization including aim to capture the local connectivity of the neurons capturing similar orientation preference (\ref{subsec:receptive_field}). The model was tested on wide range of the experimental data including white-noise and natural stimuli (\ref{subsec:stimulus}). Overall, in comparison to other state-of-the-art models it stand out with its intricately wide area of the biological properties that is uncommon in the typical models that typically capture only small portion of them (\citet{antolik2024comprehensive}.) The one property of the model that stands out is the ability to capture the spontaneous state of the neurons without external unrealistic sources of variability (\citet{antolik2024comprehensive}) that allows reproduce stimulus depended changes in trial-to-trial variability (\ref{sec:evaluation_methods}) of the neurons (\citet{baudot_animation_2013}). These properties are tested on the blank gray stimulus (\citet{PAPAIOANNOU1972558}). These properties make the model an ideal source of the artificial dataset for the development of our model as they capture the biological properties and there is almost no limitation in the amount of the data that we can generate.

On the other hand, it needs to be said that the model is not perfect and there are still some limitations. Mainly, the model is a simplification of the biological system and not every property is precisely correct. Example of this is the recurrent cortical pathway from layer II/III to layer IV that is included in the model as a simplification of the modulatory feedback from layers V and VI that are not included in the model and omision of other higher cortical modulatory pathways and variety of neural types within the modeled layers (all briefly mentioned in Section \ref{sec:v1}). 

\subsubsection{Architecture of the Model}
\label{subsubsec:spiking_cat_architecture}
Alongside with this is should be mentioned that the the model itself is significantly downsampled in comparison to biological system of the selected area. The model of V1 contains 108,150 neurons and approximately 155 million synapses that is approximately 10\% of the neuronal density in the cat's V1 (\citet{beaulie1989number}). The distribution of the neurons between the layers is equal which is corresponding to the distribution in the biological network (\citet{beaulie1989number}). Each layer contains population of excitatory and inhibitory neurons \ref{subsec:synaptic_transmission} that are present in ratio 4:1 that corresponds with the biological system (\citet{bealuliee1992quantitative, markram_interneurons_2004}). There are both feed-forward and recurrent connections included both within layers and interlayer connections from excitatory neurons to other layers.

Additionally, the model contains also the part focused on LGN (\ref{sec:lgn}) neurons. In this the ON and OFF LGN neurons pathways (Section \ref{subsec:retina}, Figures \ref{fig:early_vis_processing} and \ref{fig:on_off_cells}) are modeled separately. The LGN neurons are selected to correspond to the same receptive field as the neurons from the V1 part. The total amount of both ON and OFF LGN neurons is 7,200. For further description of the model and its properties we recommend to read the original paper from \citet{antolik2024comprehensive}.

\section{Artificial Dataset}
\label{sec:artificial_dataset}
The dataset for our model consists of the generated spiking activity of the model from \citet{antolik2024comprehensive} described in Section \ref{sec:cats_model}. The raw dataset generated from the SNN consists of the extensive description of the neuronal activity throughout the experiment duration. The dataset contains of 14,400 LGN and 108,150 V1 neuronal activity of the neurons described in the Section \ref{sec:cats_model}. The neuronal activity is in format of an array of summarized activity in time bins with resolution 1 ms. The array contains the sequence corresponding to activity in duration of the multiple experiments that will be further described in the following sections. Additionally, it is worth noting that since the original model is focused on the comprehensive description of the biological properties and thus its simulation results contain a vast amount of information that is not used in our model. Because of this, we preprocessed the dataset to remove the unnecessary information to make it more suitable for our model. The preprocessing steps will be described in the following sections of this section.

\subsection{Stimuli Sequence Description}
\label{subsec:stimulus_sequence}
The dataset is divided into multiple parts where each part correspond to a continuous sequence of neuronal responses to a given variety of the stimuli that we call \emph{stimuli sequence}. The sequence consists of pairs of intervals corresponding to \emph{blank} stage and \emph{stimulus} stage that are periodically repeating in the sequences. Where blank corresponds to the blank gray stimulus mentioned in the Section \ref{sec:cats_model} and stimulus corresponds to the natural stimuli \ref{subsec:stimulus} representing neuronal responses to some natural image. The technical details can be seen in the paper from \citet{antolik2024comprehensive}. The sequence is the following: the experiment starts with the blank stimulus for a selected time period $t_{\text{blank}}$, this is done because we want to make sure our model captures the spontaneous activity of the neurons \ref{subsec:spiking_model_description} and that the neurons does not encode any other information. After this, the natural stimulus is presented for the selected time period $t_{\text{stim}}$, so the neuronal responses correspond to this natural stimulus. After this, the sequence is repeated periodically for arbitrary number of times typically based on various technical properties of the simulation, memory efficiency, etc. and may differ. The reason why we run blank stage after each stimulus is to make sure that the model neurons reset their state to the spontaneous activity and does not encode any information from the previous stimulus. Thanks to this the next stimulus is not effected by the previous one. Additionally, we can also capture how the spontaneous activity of the neurons look like since the SNN captures the spontaneous activity as mentioned in Section \ref{subsec:spiking_model_description} and we can tract how the neuronal responses changes when the stimulus changes. What should be also mentioned is that the blank stage is not present after the last stimuli period, so we do not capture the appropriate responses to the blank stimulus after the last stimulus in the sequence. This is not basically a problem in our case since majority of the stimuli pairs contain this blank stage and only a small portion belongs to the problematic part. 

\subsection{Experiment}
\label{subsec:experiment}
In our model we work with the units that we call \emph{experiments}. The experiment is a sequence of the neuronal activity that corresponds to appropriate parts of the subsequent sequence of the blank, stimuli and blank stages and are extracted from the stimuli sequence defined in Section \ref{subsec:stimulus_sequence}. We split the stimulus sequence into smaller parts since the sequences itself contains high amount of the data and it would not be suitable in terms of memory to work with all of them as an unit of information. We exploit the properties of the blank stages as mentioned in the Section \ref{subsec:stimulus_sequence} and split the sequence right in the middle of the blank stimuli. We apply this split as we expect that in the middle of the blank stage the neuronal responses have already reset enough to capture only spontaneous activity and is not affected by the previous natural stimuli. Thanks to this we then have a units of information in uniform format (second half of the blank stage, natural stimulus stage and the first half of the following blank stage) that we will use in our dataset for our model. We will call it \emph{experiment} and formally define them as:

\begin{defn}[Experiment]
    Let $S$ be a stimuli sequence of blank and natural stimuli pairs $(b_i, s_i)$ periodically repeating in $N \in \N$ repetitions where $i \in [1, 2, \dots, N]$ specifies the order of the pair in the sequence. Let $l_{b}, l_{s} \in \N$ be the lengths of the blank respectively natural stimuli. Lets denote $b_i[0:\frac{l_b}{2}]$ as first $\frac{l_b}{2}$ (half) time steps of the blank sequence $b_i$ and $b_i[\frac{l_b}{2}:l_b]$ as the rest $\frac{l_b}{2}$ (second half) time steps. Then the \textbf{experiment} is defined for each stimuli pair with index $j \in [1, 2, \dots, (N-1)]$ as an ordered sequence:

    $$\left(b_i\left[\frac{l_b}{2}:l_b\right]; s_i; b_{i+1}\left[0:\frac{l_b}{2}\right]\right)$$

    And for the last pair in the sequence:

    $$\left(b_{N}\left[\frac{l_b}{2}:l_b\right]; s_{N}\right)$$
\end{defn}
\label{def:experiment}

It is worth noting that we do not use the first half of the first blank sequence to have the information in a uniform format and to facilitate the work with the dataset in our model. This means that we lose a some information from the original dataset but since we still capture the blank stage and only omit the first half of it we assume that the effect of this part of the dataset on the system identification task would be minimal. Also the last experiment from each stimuli sequence have missing subsequent blank stage. This is not ideal since we lose some information about the neuronal responses in the following blank stage. In our case it is not that problematic since number of these experiments is a minority of the dataset and in our model we pad these missing sequences to ensure the experiment duration is uniform across all experiments. These steps would be further described in the following parts of this section. For illustration we also provide the schema of the stimuli sequence and its split into the separate experiments that can be seen in Figure (TODO: add figure of experiment sequence).

\subsection{Dataset Preprocessing}
\label{subsec:dataset_preprocess}
It has been already mentioned that the original SNN from \citet{antolik2024comprehensive} focuses on the comprehensive description of the neuronal activity, so, the original dataset that we obtain from this model simulations is unnecessary complex for our purposes and we need to preprocess it and trim it to the format that is suitable for our model. The main reason for this preprocessing is the memory and time efficiency as the DNN models itself are memory and time demanding (as mentioned in \ref{subsec:deep_learning_approach}) and it would be unsuitable to work with the raw dataset extracted from the SNN run.

Our DNN model that we will describe in the following section is based on the DNNs that predicts neuronal spiking activity in V1 in the given time step from the spiking activity of the appropriate input neurons. For training of the model we then need only the spike trains (\ref{subsec:spike_trains}) of the neurons and other information then can be omitted.

In the preprocessing step, we then extract the spike trains of all the neurons in all the stimuli sequences (\ref{subsec:stimulus_sequence}). Followingly, we then split these sequences to the experiments (\ref{subsec:experiment}). As a result we get the final dataset in the matrix representation.

\subsection{Dataset Description}
\label{subsec:dataset_description}

After the raw dataset generated from the model from \citet{antolik2024comprehensive} undergo the preprocessing steps \ref{subsec:dataset_preprocess} we get the final dataset used in our model. The unit of the dataset is the experiment \ref{subsec:experiment}. One experiment data are split into 6 matrices representing different neuronal population spike trains. Two of them represent ON respectively OFF neurons in LGN that we label as X\_ON and X\_OFF, the rest of the neurons are split to layer IV and II/III neurons that are additionally split into inhibitory and excitatory neurons we will label these as V1\_Exc\_L4, V1\_Inh\_L4, V1\_Exc\_L23 and V1\_Inh\_L23. Number of neurons in each population are described in the Section \ref{subsubsec:spiking_cat_architecture} and correspond to number of neurons from the model of \citet{antolik2024comprehensive}.

The dataset matrix for a selected neuronal population is $M = \R^{N \times M \times T}$ where $N$ is a number of experiments, $M$ is a number of neurons in the population and $T$ is the number of time steps in the experiment (see Section \ref{subsec:experiment} for a comprehensive description of the experiment definition).

In the case of our artificial dataset the duration of the blank stage is 150ms and the duration of the natural stimuli stage is 560ms both stages are measured in 1ms time bins. This thus mean that data from 1 experiment for one selected neuron consist of the spike train \ref{subsec:spike_trains} of the corresponding neuron that is encoded in the binary format where 1 represents that the spike occured in the given time step and 0 that the spike did not happen. From the definition of the experiment \ref{subsec:experiment} the time steps are sorted in the given sequence. Last 75ms of the blank stage, followed by the 560ms of the natural stimuli sequence and followed by the first 75ms of the following blank stage. As mentioned in the definition of the experiment \ref{def:experiment} and the Section \ref{subsec:experiment} there are a few experiments from the end of the stimuli sequence that does not have the following blank sequence. In order to maintain the same length of the experiment duration we pad these sequences after the natural stimuli to the appropriate sequence length with 0, in other words with no spikes at all. Although this approach might not be ideal and not clearly biologically correct, as there is spontaneous spiking activity happening \ref{subsec:spiking_model_description} and thus the spike trains in biological example does not contain only 0 (no spikes at all). We are conscious of this problem but since it is the case only for the minority of the experiments we leave it as it is as the biologically relevant solution would add more complexity to the model solution alongside with the lowering computational efficiency of our model.

In classical machine learning approach it is typical to split the dataset into training, evaluation and test datasets ideally in several randomized splits. In our case there is a complication that refrains us from doing so and this is the trial-to-trial variability of the neuronal responses that leads to necessity of the multiple trials in evaluation and testing dataset which is closely described in the Section \ref{sec:evaluation_methods}. The fact that multiple trials distinctness is mainly influenced by the spontaneous neuronal activity as mentioned in \ref{sec:cats_model} and more profoundly in \citet{antolik2024comprehensive} we decided to not include multiple trials in our training dataset. This decision has been done because we assume that for the training of our model would be more beneficial to generate more variants of the responses to the distinct natural stimuli than instead of that include more than 1 trial as the spontaneous activity information is also included in the blank stimuli sequences \ref{sec:cats_model}. This decision has been made while including multiple aspects of the model training such as size of the dataset and memory storage, speed of the model training, time necessity of the dataset generation etc. As a result we thus cannot use different variants of the dataset splits.

\subsubsection{Train Dataset}
\label{subsubsec:train_dataset}

The raw train dataset consists of the stimuli sequences that contain 100 pairs of blank and natural stimuli. All of them has been presented only in 1 trial. The total number of these sequences is 500 which means that after the preprocessing steps \ref{subsec:dataset_preprocess} and dataset split to the experiments \ref{subsec:experiment} the train dataset consists of neuronal responses for the 50,000 of experiments.

\subsubsection{Test and Validation Dataset}
\label{subsubsec:test_dataset}

The second part of our dataset is much smaller and is dedicated to serve as validation or test dataset. It consists 18 stimuli sequences where each contain 50 pairs of blank and natural stimuli sequences. Each stimuli has been presented in 20 trials. After the preprocessing steps there has been total number of 900 experiments with 20 trial repetitions. We have decided to split randomly the subset to validation dataset and test dataset in ratio 1:9.

\subsubsection{Dataset Subset Selection}
\label{subsubsec:subset_selection}

As our aim is to develop the biologically plausible RNN model of the same system as is used in the paper from \citet{antolik2024comprehensive}, it would require to design the RNN model where each neuron corresponds to one neuron from the artificial network. Since we do not have any synaptic constraints we connect those layers in all-to-all manner which leads to high amount of parameters and non-suitable memory consumption. In order to facilitate the development process and training of the model we have decided to randomly select only 10\% of neurons from each layer. This makes our model more suitable for testing and fine-tuning, however, we should keep in mind that the neurons subset select might affect the model performance and also that the smaller subset might also affect the model performance as we do not measure it on all of the provided neurons. To eliminate these problems as much as possible we run the experiments in parallel for 20 different randomly selected subsets of neurons. In addition it is worth noting that while trying to cross run the models with the higher number of neurons it has been empirically decided that the results using the subset of 10\% of the neurons does not behave significantly differentaly in comparison to the models with higher partition of the neurons. Further analysis has been done in the following chapter.
